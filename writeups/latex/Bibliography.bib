
@book{bergsma_testing_2004,
	title = {Testing conditional independence for continuous random variables},
	publisher = {Eurandom},
	author = {Bergsma, Wicher Pieter},
	date = {2004},
	keywords = {Technical}
}

@article{flaxman_scalable_2018,
	title = {Scalable high-resolution forecasting of sparse spatiotemporal events with kernel methods: A winning solution to the {NIJ} Real-Time Crime Forecasting Challenge},
	url = {http://arxiv.org/abs/1801.02858},
	shorttitle = {Scalable high-resolution forecasting of sparse spatiotemporal events with kernel methods},
	abstract = {We propose a generic spatiotemporal event forecasting method, which we developed for the National Institute of Justice's ({NIJ}) Real-Time Crime Forecasting Challenge. Our solution to the challenge is a spatiotemporal forecasting model combining scalable randomized Reproducing Kernel Hilbert Space ({RKHS}) methods for approximating Gaussian processes with autoregressive smoothing kernels in a regularized supervised learning framework. While the smoothing kernels capture the two main approaches in current use in the field of crime forecasting, heatmap-based (kernel density estimation) and self-exciting point process ({SEPP}) models, the {RKHS} component of the model can be understood as an approximation to the popular log-Gaussian Cox Process model. For inference, we discretize the spatiotemporal point pattern and learn a log intensity function using the Poisson likelihood and highly efficient gradient-based optimization methods. Model hyperparameters including quality of {RKHS} approximation, spatial and temporal kernel lengthscales, number of autoregressive lags, bandwidths for smoothing kernels, as well as cell shape, size, and rotation, were learned using temporal crossvalidation. Resulting predictions significantly exceeded baseline {KDE} estimates; we had winning submissions to the competition in each of the different crime types and across different timescales, suggesting that our method is generically applicable.},
	journaltitle = {{arXiv}:1801.02858 [stat]},
	author = {Flaxman, Seth and Chirico, Michael and Pereira, Pau and Loeffler, Charles},
	urldate = {2019-03-14},
	date = {2018-01-09},
	eprinttype = {arxiv},
	eprint = {1801.02858},
	keywords = {Statistics - Applications, Statistics - Machine Learning, Technical}
}

@article{richardson_dirty_2019,
	title = {Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice},
	volume = {Forthcoming},
	url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333423},
	pages = {30},
	journaltitle = {New York University Law Review Online},
	author = {Richardson, Rashida and Schultz, Jason and Crawford, Kate},
	date = {2019-03-07},
	langid = {english},
	keywords = {Legal}
}

@article{kilbertus_avoiding_2017,
	title = {Avoiding Discrimination through Causal Reasoning},
	url = {http://arxiv.org/abs/1706.02744},
	abstract = {Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.},
	journaltitle = {{arXiv}:1706.02744 [cs, stat]},
	author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Schölkopf, Bernhard},
	urldate = {2018-09-22},
	date = {2017-06-08},
	eprinttype = {arxiv},
	eprint = {1706.02744},
	note = {bibtex:kilbertus\_avoiding\_through\_causality\_2017},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Technical}
}

@unpublished{narayanan_21_2018,
	location = {New York, {NY}, {USA}},
	title = {21 Fairness Definitions and Their Politics},
	url = {https://www.youtube.com/watch?v=jIXIuYdnyyk},
	abstract = {Computer scientists and statisticians have devised numerous mathematical criteria to define what it means for a classifier or a model to be fair. The proliferation of these definitions represents an attempt to make technical sense of the complex, shifting social understanding of fairness. Thus, these definitions are laden with values and politics, and seemingly technical discussions about mathematical definitions in fact implicate weighty normative questions. A core component of these technical discussions has been the discovery of trade-offs between different (mathematical) notions of fairness; these trade-offs deserve attention beyond the technical community.

This tutorial has two goals. The first is to explain the technical definitions. In doing so, I will aim to make explicit the values embedded in each of them. This will help policymakers and others better understand what is truly at stake in debates about fairness criteria (such as individual fairness versus group fairness, or statistical parity versus error-rate equality). It will also help computer scientists recognize that the proliferation of definitions is to be celebrated, not shunned, and that the search for one true definition is not a fruitful direction, as technical considerations cannot adjudicate moral debates.

My second goal is to highlight technical observations and discoveries that deserve broader consideration. Many of these can be seen as “trolley problems” for algorithmic fairness​, and beg to be connected to philosophical theories of justice. I hope to make it easier for ethics scholars, philosophers, and domain experts to approach this territory.},
	type = {Tutorial},
	howpublished = {Tutorial},
	note = {Conference on Fairness, Accountability, and  Transparency ({FAT}*)},
	author = {Narayanan, Arvind},
	date = {2018-02-23},
	keywords = {Technical}
}

@online{noauthor_google_nodate,
	title = {Google, cancel Project Dragonfly!},
	url = {https://actions.sumofus.org/a/google-cancel-project-dragonfly-1},
	abstract = {Right now at least a million people are being detained in mass internment camps by the Chinese authorities simply because they’re Muslim. 

And yet, Google is currently developing a censored search engine for China. 

Google is helping the Chinese government whitewash its brutal human rights record --  and even Google’s own employees are fighting back.},
	titleaddon = {{SumOfUs}},
	urldate = {2018-12-12},
	langid = {english},
	note = {bibtex:sumofus\_google\_petition}
}

@inproceedings{mohler_penalized_2018,
	title = {A penalized likelihood method for balancing accuracy and fairness in predictive policing},
	eventtitle = {{IEEE} International Conference on Systems, Man, and Cybernetics ({SMC}2018)},
	pages = {6},
	author = {Mohler, G. O. and Raje, Rajeev and Valasik, Matthew and Carter, Jeremy and Brantingham, P Jeffrey},
	date = {2018},
	langid = {english},
	keywords = {Technical}
}

@article{mohler_marked_2014,
	title = {Marked point process hotspot maps for homicide and gun crime prediction in Chicago},
	volume = {30},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207014000284},
	doi = {10.1016/j.ijforecast.2014.01.004},
	abstract = {Crime hotspot maps are a widely used and successful method of displaying spatial crime patterns and allocating police resources. However, hotspot maps are often created over a single timescale using only one crime type. In the case of short-term hotspot maps that utilize several weeks of crime data, risk estimates suffer from a high variance, especially for low frequency crimes such as homicide. Long-term hotspot maps that utilize several years of data fail to take into account near-repeat effects and emerging hotspot trends. In this paper we show how point process models of crime can be extended to include leading indicator crime types, while capturing both short-term and long-term patterns of risk, through a marked point process approach. Several years of data and many different crime types are systematically combined to yield accurate hotspot maps that can be used for the purpose of predictive policing of gun-related crime. We apply the methodology to a large, open source data set which has been made available to the general public online by the Chicago Police Department.},
	pages = {491--497},
	number = {3},
	journaltitle = {International Journal of Forecasting},
	shortjournal = {International Journal of Forecasting},
	author = {Mohler, G. O.},
	urldate = {2018-09-18},
	date = {2014-07-01},
	keywords = {Crime hotspot, Expectation-maximization, Leading indicators, Marked point process, Technical}
}

@article{singh_fairness_2018,
	title = {Fairness of Exposure in Rankings},
	url = {http://arxiv.org/abs/1802.07281},
	doi = {10.1145/3219819.3220088},
	abstract = {Rankings are ubiquitous in the online world today. As we have transitioned from finding books in libraries to ranking products, jobs, job applicants, opinions and potential romantic partners, there is a substantial precedent that ranking systems have a responsibility not only to their users but also to the items being ranked. To address these often conflicting responsibilities, we propose a conceptual and computational framework that allows the formulation of fairness constraints on rankings in terms of exposure allocation. As part of this framework, we develop efficient algorithms for finding rankings that maximize the utility for the user while provably satisfying a specifiable notion of fairness. Since fairness goals can be application specific, we show how a broad range of fairness constraints can be implemented using our framework, including forms of demographic parity, disparate treatment, and disparate impact constraints. We illustrate the effect of these constraints by providing empirical results on two ranking problems.},
	pages = {2219--2228},
	journaltitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining  - {KDD} '18},
	author = {Singh, Ashudeep and Joachims, Thorsten},
	urldate = {2019-02-11},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1802.07281},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Retrieval, Technical}
}

@article{zehlike_fa*ir:_2017,
	title = {{FA}*{IR}: A Fair Top-k Ranking Algorithm},
	url = {http://arxiv.org/abs/1706.06368},
	doi = {10.1145/3132847.3132938},
	shorttitle = {{FA}*{IR}},
	abstract = {In this work, we define and solve the Fair Top-k Ranking problem, in which we want to determine a subset of k candidates from a large pool of n {\textgreater}{\textgreater} k candidates, maximizing utility (i.e., select the "best" candidates) subject to group fairness criteria. Our ranked group fairness definition extends group fairness using the standard notion of protected groups and is based on ensuring that the proportion of protected candidates in every prefix of the top-k ranking remains statistically above or indistinguishable from a given minimum. Utility is operationalized in two ways: (i) every candidate included in the top-\$k\$ should be more qualified than every candidate not included; and (ii) for every pair of candidates in the top-k, the more qualified candidate should be ranked above. An efficient algorithm is presented for producing the Fair Top-k Ranking, and tested experimentally on existing datasets as well as new datasets released with this paper, showing that our approach yields small distortions with respect to rankings that maximize utility without considering fairness criteria. To the best of our knowledge, this is the first algorithm grounded in statistical tests that can mitigate biases in the representation of an under-represented group along a ranked list.},
	pages = {1569--1578},
	journaltitle = {Proceedings of the 2017 {ACM} on Conference on Information and Knowledge Management  - {CIKM} '17},
	author = {Zehlike, Meike and Bonchi, Francesco and Castillo, Carlos and Hajian, Sara and Megahed, Mohamed and Baeza-Yates, Ricardo},
	urldate = {2019-02-11},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1706.06368},
	keywords = {Computer Science - Computers and Society, Computer Science - Information Retrieval, H.3.3, J.1, Technical}
}

@article{saunders_predictions_2016,
	title = {Predictions put into practice: a quasi-experimental evaluation of Chicago’s predictive policing pilot},
	volume = {12},
	issn = {1572-8315},
	url = {https://doi.org/10.1007/s11292-016-9272-0},
	doi = {10.1007/s11292-016-9272-0},
	shorttitle = {Predictions put into practice},
	abstract = {{ObjectivesIn} 2013, the Chicago Police Department conducted a pilot of a predictive policing program designed to reduce gun violence. The program included development of a Strategic Subjects List ({SSL}) of people estimated to be at highest risk of gun violence who were then referred to local police commanders for a preventive intervention. The purpose of this study is to identify the impact of the pilot on individual- and city-level gun violence, and to test possible drivers of results.{MethodsThe} {SSL} consisted of 426 people estimated to be at highest risk of gun violence. We used {ARIMA} models to estimate impacts on city-level homicide trends, and propensity score matching to estimate the effects of being placed on the list on five measures related to gun violence. A mediation analysis and interviews with police leadership and {COMPSTAT} meeting observations help understand what is driving results.{ResultsIndividuals} on the {SSL} are not more or less likely to become a victim of a homicide or shooting than the comparison group, and this is further supported by city-level analysis. The treated group is more likely to be arrested for a shooting.{ConclusionsIt} is not clear how the predictions should be used in the field. One potential reason why being placed on the list resulted in an increased chance of being arrested for a shooting is that some officers may have used the list as leads to closing shooting cases. The results provide for a discussion about the future of individual-based predictive policing programs.},
	pages = {347--371},
	number = {3},
	journaltitle = {Journal of Experimental Criminology},
	shortjournal = {J Exp Criminol},
	author = {Saunders, Jessica and Hunt, Priscillia and Hollywood, John S.},
	urldate = {2019-02-11},
	date = {2016-09-01},
	langid = {english},
	keywords = {Predictive policing, Program evaluation, Propensity score matching, Quasi-experimental design, Risk assessment, Technical, Time series analysis}
}

@article{mohler_randomized_2015,
	title = {Randomized Controlled Field Trials of Predictive Policing},
	volume = {110},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2015.1077710},
	doi = {10.1080/01621459.2015.1077710},
	abstract = {The concentration of police resources in stable crime hotspots has proven effective in reducing crime, but the extent to which police can disrupt dynamically changing crime hotspots is unknown. Police must be able to anticipate the future location of dynamic hotspots to disrupt them. Here we report results of two randomized controlled trials of near real-time epidemic-type aftershock sequence ({ETAS}) crime forecasting, one trial within three divisions of the Los Angeles Police Department and the other trial within two divisions of the Kent Police Department (United Kingdom). We investigate the extent to which (i) {ETAS} models of short-term crime risk outperform existing best practice of hotspot maps produced by dedicated crime analysts, (ii) police officers in the field can dynamically patrol predicted hotspots given limited resources, and (iii) crime can be reduced by predictive policing algorithms under realistic law enforcement resource constraints. While previous hotspot policing experiments fix treatment and control hotspots throughout the experimental period, we use a novel experimental design to allow treatment and control hotspots to change dynamically over the course of the experiment. Our results show that {ETAS} models predict 1.4–2.2 times as much crime compared to a dedicated crime analyst using existing criminal intelligence and hotspot mapping practice. Police patrols using {ETAS} forecasts led to an average 7.4\% reduction in crime volume as a function of patrol time, whereas patrols based upon analyst predictions showed no significant effect. Dynamic police patrol in response to {ETAS} crime forecasts can disrupt opportunities for crime and lead to real crime reductions.},
	pages = {1399--1411},
	number = {512},
	journaltitle = {Journal of the American Statistical Association},
	author = {Mohler, G. O. and Short, M. B. and Malinowski, Sean and Johnson, Mark and Tita, G. E. and Bertozzi, Andrea L. and Brantingham, P. J.},
	urldate = {2019-02-11},
	date = {2015-10-02},
	keywords = {Crime, Experimental methods, Machine learning, Point processes, Policing dosage, Technical}
}

@report{robinson_stuck_2016,
	title = {Stuck in a Pattern: Early evidence on "predictive policing" and civil rights},
	url = {https://www.upturn.org/static/reports/2016/stuck-in-a-pattern/files/Upturn_-_Stuck_In_a_Pattern_v.1.01.pdf},
	abstract = {The term “predictive policing” refers to computer systems that use data to forecast where crime will happen or who will be involved. Some tools produce maps of anticipated crime “hot spots,” while others score and flag people deemed most likely to be involved in crime or violence.

Though these systems are rolling out in police departments nationwide, our research found pervasive, fundamental gaps in what’s publicly known about them.
How these tools work and make predictions, how they define and measure their performance and how police departments actually use these systems day-to-day, are all unclear. Further, vendors routinely claim that the inner working of their technology is proprietary, keeping their methods a closely-held trade secret, even from the departments themselves. And early research findings suggest that these systems may not actually make people safer — and that they may lead to even more aggressive enforcement in communities that are already heavily policed.

Predictive policing systems typically rely, at a minimum, on historical data held by the police — records of crimes reported by the community, and of those identified by police on patrol, for example. Some systems seek to enhance their predictions by considering other factors, like the weather or a location’s proximity to liquor stores. However, criminologists have long emphasized that crime reports, and other statistics gathered by the police, are not an accurate record of all the crime that occurs in a community; instead, they are partly a record of law enforcement’s responses to what happens in a community. This means that predictive systems that rely on historical crime data risk fueling a cycle of distorted enforcement.

Predictions that come from computers may be trusted too much by police, the courts, and the public. People who lack technical expertise have a natural and well-documented tendency to overestimate the accuracy, objectivity, and reliability of information that comes from a computer, including from a predictive policing system. As one {RAND} study aptly put it, “[p]redictive policing has been so hyped that the reality cannot live up to the hyperbole. There is an underlying, erroneous assumption that advanced mathematical and computational power is both necessary and sufficient to reduce crime [but in fact] the predictions are only as good as the data used to make them.”1

The fact that we even call these systems “predictive” is itself a telling sign of excessive confidence in the systems. The systems really make general forecasts, not specific predictions. A more responsible term — and one more accurately evocative of the uncertainty inherent in these systems, would be “forecasting.”

The systems we found also appear not to track details about enforcement practices or community needs, which means that departments are missing potentially powerful opportunities to assess their performance more holistically and to avoid problems within their ranks.

In an overwhelming majority of cases, departments operate predictive systems with no apparent governing policies, and open public discussion about the adoption of these systems seems to be the exception to the rule. Though federal and state grant money has helped fuel the adoption of these systems, that money comes with few real strings in terms of transparency, accountability, and meaningfully involving the public.

In our survey of the nation’s 50 largest police forces, we found that at least 20 of them have used a predictive policing system, with at least an additional 11 actively exploring options to do so. Yet some sources indicate that 150 or more departments may be moving toward these systems with pilots, tests, or new deployments.

Our study finds a number of key risks in predictive policing, and a trend of rapid, poorly informed adoption in which those risks are often not considered. We believe that conscientious application of data has the potential to improve police practices in the future. But we found little evidence that today’s systems live up to their claims, and significant reason to fear that they may reinforce disproportionate and discriminatory policing practices.},
	pages = {29},
	institution = {Upturn},
	author = {Robinson, David and Koepke, Logan},
	urldate = {2019-01-16},
	date = {2016-08},
	note = {v1.0.1},
	keywords = {{NonTechnical}}
}

@book{perry_predictive_2013,
	location = {Santa Monica, {CA}},
	title = {Predictive policing: the role of crime forecasting in law enforcement operations},
	isbn = {978-0-8330-8148-3},
	shorttitle = {Predictive policing},
	pagetotal = {155},
	publisher = {{RAND}},
	author = {Perry, Walt L.},
	date = {2013},
	langid = {english},
	keywords = {Crime forecasting, Crime prevention, Criminal behavior, Prediction of, {NonTechnical}, Police, United States}
}

@book{barocas_fairness_2018,
	title = {Fairness and Machine Learning},
	url = {http://www.fairmlbook.org},
	publisher = {fairmlbook.org},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	date = {2018},
	keywords = {Technical}
}

@online{stroud_does_2016,
	title = {Does predictive policing actually work?},
	url = {https://www.theverge.com/2016/5/4/11583204/official-police-business-predictive-policing-paper},
	abstract = {Crime forecasting tools are taking off, but good data is hard to find.},
	titleaddon = {The Verge},
	author = {Stroud, Matt},
	urldate = {2019-01-21},
	date = {2016-05-04},
	keywords = {{NonTechnical}}
}

@report{benslimane_etude_2014,
	title = {Étude critique d’un système d’analyse prédictive appliqué à la criminalité : Predpol®},
	url = {https://cortecs.org/wp-content/uploads/2014/10/rapport_stage_Ismael_Benslimane.pdf},
	pages = {4},
	author = {Benslimane, Ismael},
	date = {2014-06-18},
	langid = {french},
	keywords = {Technical}
}

@article{degeling_what_2018,
	title = {What is wrong about Robocops as consultants? A technology-centric critique of predictive policing},
	volume = {33},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-017-0730-7},
	doi = {10.1007/s00146-017-0730-7},
	shorttitle = {What is wrong about Robocops as consultants?},
	abstract = {Fighting crime has historically been a field that drives technological innovation, and it can serve as an example of different governance styles in societies. Predictive policing is one of the recent innovations that covers technical trends such as machine learning, preventive crime fighting strategies, and actual policing in cities. However, it seems that a combination of exaggerated hopes produced by technology evangelists, media hype, and ignorance of the actual problems of the technology may have (over-)boosted sales of software that supports policing by predicting offenders and crime areas. In this paper we analyse currently used predictive policing software packages with respect to common problems of data mining, and describe challenges that arise in the context of their socio-technical application.},
	pages = {347--356},
	number = {3},
	journaltitle = {{AI} \& Society},
	shortjournal = {{AI} \& Soc},
	author = {Degeling, Martin and Berendt, Bettina},
	urldate = {2019-01-16},
	date = {2018-08-01},
	langid = {english},
	keywords = {Big data, Data mining, {NonTechnical}, Predictive policing, Privacy}
}

@article{king_dangers_2006,
	title = {The Dangers of Extreme Counterfactuals},
	volume = {14},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700001340/type/journal_article},
	doi = {10.1093/pan/mpj004},
	pages = {131--159},
	number = {2},
	journaltitle = {Political Analysis},
	author = {King, Gary and Zeng, Langche},
	urldate = {2018-12-20},
	date = {2006},
	langid = {english}
}

@incollection{coady_epistemic_2017,
	location = {London},
	edition = {1st},
	title = {Epistemic Justice as Distributive Justice},
	isbn = {978-1-351-81450-8 978-1-315-21204-3},
	url = {https://www-taylorfrancis-com.ccl.idm.oclc.org/books/9781351814508},
	abstract = {Is epistemic injustice a form of distributive injustice? In her early, profoundly influential work on epistemic injustice Miranda Fricker makes it clear that she does not think it is. In a recent article, however, she has expanded her conception of epistemic injustice to include something she calls distributive epistemic injustice, which she characterizes as “the unfair distribution of epistemic goods such as education or information” (Fricker 2013: 1318). She contrasts this with her earlier, narrower, conception of the subject, which she now prefers to call discriminatory epistemic injustice. In what follows I will challenge Fricker’s distinction between discriminatory and distributive epistemic injustice; each of the forms of epistemic injustice that Fricker describes is a form of distributive injustice (or at any rate can be fruitfully treated as such) and that considerable insight into the nature of these injustices, and into the interrelations between them, can be gained from recognizing this fact.
Epistemic injustice in the original, and still most widely used, sense of the term is divided by Fricker into two categories: testimonial injustice and hermeneutic injustice. I will consider them separately.},
	pages = {61--68},
	booktitle = {The Routledge Handbook of Epistemic Injustice},
	publisher = {Routledge},
	author = {Coady, David},
	editor = {Kidd, Ian James and Medina, José and Pohlhaus, Gaile M},
	urldate = {2018-12-18},
	date = {2017},
	note = {{OCLC}: 1015178339}
}

@incollection{origgi_epistemic_2017,
	location = {London},
	edition = {1st},
	title = {Epistemic Justice: The Case of Digital Environments},
	isbn = {978-1-351-81450-8 978-1-315-21204-3},
	url = {https://www-taylorfrancis-com.ccl.idm.oclc.org/books/9781351814508},
	abstract = {In her insightful and worldly acclaimed work on epistemic injustice, Miranda Fricker argues that people can be distinctively wronged in their capacity as knowers. Much of the discussion around the notion of epistemic injustice has revolved around power relations between different groups of people. In this chapter we would like to take a different perspective on epistemic injustice, by applying it to the context of human/{ICT} interactions. New technologies may be a source of epistemic harm by depriving people of their credibility about themselves. The massive gathering of big data about our own identity and behaviour creates a new asymmetry of power between algorithms and humans: algorithms are perceived today as being better knowers of ourselves than we are, thus weakening our entitlement to be credible about ourselves. We argue that these new cases of epistemic injustice are, under many aspects, more centrally epistemic than other cases described in the literature because they wrong us directly in our epistemic capacities and not only in our dignity as knowledge givers. The examples of epistemic harm we will discuss undermine our epis- temic confidence about our self-knowledge, a kind of knowledge that has been considered for a long time as markedly different from all other kinds of knowledge because of its infallibility and self-presentness. We are diminished as knowers, especially in the most intimate part of our epistemic competence. This is the case for both kinds of injustice that Fricker defines: testimonial and hermeneutical. But, before presenting a specific case, we would like to explain why we think that the {ICT} examples are more centrally epistemic than other case analyses in the literature and how they may help to contribute to answer some objections raised about the “epistemic” nature of the injustice committed towards knowledge givers and to illustrate in a clearer way the idea of epistemic objectification.},
	pages = {303--312},
	booktitle = {The Routledge Handbook of Epistemic Injustice},
	publisher = {Routledge},
	author = {Origgi, Gloria and Ciranna, Serena},
	editor = {Kidd, Ian James and Medina, José and Pohlhaus, Gaile M},
	urldate = {2018-12-18},
	date = {2017},
	note = {{OCLC}: 1015178339}
}

@collection{kidd_routledge_2017,
	location = {London},
	edition = {1st},
	title = {The Routledge Handbook of Epistemic Injustice},
	isbn = {978-1-351-81450-8 978-1-315-21204-3},
	url = {https://www-taylorfrancis-com.ccl.idm.oclc.org/books/9781351814508},
	abstract = {In the era of information and communication, issues of misinformation and miscommunication are more pressing than ever. Epistemic injustice - one of the most important and ground-breaking subjects to have emerged in philosophy in recent years - refers to those forms of unfair treatment that relate to issues of knowledge, understanding, and participation in communicative practices.

The Routledge Handbook of Epistemic Injustice is an outstanding reference source to the key topics, problems and debates in this exciting subject. The first collection of its kind, it comprises over thirty chapters by a team of international contributors, divided into five parts:

Core Concepts
Liberatory Epistemologies and Axes of Oppression
Schools of Thought and Subfields within Epistemology
Socio-political, Ethical, and Psychological Dimensions of Knowing
Case Studies of Epistemic Injustice.
As well as fundamental topics such as testimonial and hermeneutic injustice and epistemic trust, the Handbook includes chapters on important issues such as social and virtue epistemology, objectivity and objectification, implicit bias, and gender and race. Also included are chapters on areas in applied ethics and philosophy, such as law, education, and healthcare.

The Routledge Handbook of Epistemic Injustice is essential reading for students and researchers in ethics, epistemology, political philosophy, feminist theory, and philosophy of race. It will also be very useful for those in related fields, such as cultural studies, sociology, education and law.},
	pagetotal = {456},
	publisher = {Routledge},
	editor = {Kidd, Ian James and Medina, José and Pohlhaus, Gaile M},
	urldate = {2018-12-18},
	date = {2017},
	note = {{OCLC}: 1015178339}
}

@incollection{roessler_should_2015,
	location = {Cambridge},
	title = {Should personal data be a tradable good? On the moral limits of markets in privacy},
	isbn = {978-1-107-28055-7},
	url = {https://www.cambridge.org/core/product/identifier/9781107280557%23CN-bp-8/type/book_part},
	shorttitle = {Should personal data be a tradable good?},
	pages = {141--161},
	booktitle = {Social Dimensions of Privacy: Interdisciplinary Perspectives},
	publisher = {Cambridge University Press},
	author = {Roessler, Beate},
	editor = {Roessler, Beate and Mokrosinska, Dorota},
	urldate = {2018-12-19},
	date = {2015},
	doi = {10.1017/CBO9781107280557.009}
}

@incollection{posner_data_2018,
	location = {Princeton},
	title = {Data as Labor: Valuing Individual Contributions to the Economy},
	isbn = {978-0-691-17750-2},
	abstract = {"Many blame today's economic inequality, stagnation, and political instability on the free market. The solution is to rein in the market, right? [This book] turns this thinking--and pretty much all conventional thinking about markets, both for and against--on its head. The book reveals...new ways to organize markets for the good of everyone. It shows how the emancipatory force of genuinely open, free, and competitive markets can reawaken the dormant nineteenth-century spirit of liberal reform and lead to greater equality, prosperity, and cooperation. [The authors] demonstrate why private property is inherently monopolistic, and how we would all be better off if private ownership were converted into a public auction for public benefit. They show how the principle of one person, one vote inhibits democracy, suggesting instead an ingenious way for voters to effectively influence the issues that matter most to them. They argue that every citizen of a host country should benefit from immigration--not just migrants and their capitalist employers. They propose leveraging antitrust laws to liberate markets from the grip of institutional investors and creating a data labor movement to force digital monopolies to compensate people for their electronic data. Only by radically expanding the scope of markets can we reduce inequality, restore robust economic growth, and resolve political conflicts. But to do that, we must replace our most sacred institutions with truly free and open competition--[this book] shows how."--},
	pages = {205--249},
	booktitle = {Radical Markets: Uprooting Capitalism and Democracy for a Just Society},
	publisher = {Princeton University Press},
	author = {Posner, Eric A. and Weyl, E. Glen},
	date = {2018},
	note = {{OCLC}: on1030268293},
	keywords = {Capitalism, Competition, Democracy, Demokratie, Eigentum, Free enterprise, Kapitalismus, Nonfiction, Ordnungstheorie, Right of property}
}

@article{satz_markets_1995,
	title = {Markets in Women's Sexual Labor},
	volume = {106},
	issn = {0014-1704},
	url = {http://www.jstor.org/stable/2382005},
	pages = {63--85},
	number = {1},
	journaltitle = {Ethics},
	author = {Satz, Debra},
	urldate = {2018-12-18},
	date = {1995}
}

@incollection{decew_privacy_2018,
	edition = {Spring 2018},
	title = {Privacy},
	url = {https://plato.stanford.edu/archives/spr2018/entries/privacy/},
	abstract = {The term “privacy” is used frequently in ordinarylanguage as well as in philosophical, political and legal discussions,yet there is no single definition or analysis or meaning of the term.The concept of privacy has broad historical roots in sociological andanthropological discussions about how extensively it is valued andpreserved in various cultures. Moreover, the concept has historicalorigins in well known philosophical discussions, most {notablyAristotle}’s distinction between the public sphere of political activityand the private sphere associated with family and domestic life. Yethistorical use of the term is not uniform, and there remains confusionover the meaning, value and scope of the concept of privacy., Early treatises on privacy appeared with the development of privacyprotection in American law from the 1890s onward, and privacyprotection was justified largely on moral grounds. This literaturehelps distinguish descriptive accounts of privacy, describingwhat is in fact protected as private, from normative accountsof privacy defending its value and the extent to which it should beprotected. In these discussions some treat privacy as aninterest with moral value, while others refer to it as a moralor legal right that ought to be protected by society or thelaw. Clearly one can be insensitive to another’s privacy interestswithout violating any right to privacy, if there is one., There are several skeptical and critical accounts of privacy.According to one well known argument there is no right to privacy andthere is nothing special about privacy, because any interest protectedas private can be equally well explained and protected by otherinterests or rights, most notably rights to property and bodilysecurity (Thomson, 1975). Other critiques argue that privacy interestsare not distinctive because the personal interests they protect areeconomically inefficient (Posner, 1981) or that they are not groundedin any adequate legal doctrine (Bork, 1990). Finally, there is thefeminist critique of privacy, that granting special status to privacyis detrimental to women and others because it is used as a shield todominate and control them, silence them, and cover up abuse ({MacKinnon},1989)., Nevertheless, most theorists take the view that privacy is ameaningful and valuable concept. Philosophical debates concerningdefinitions of privacy became prominent in the second half of thetwentieth century, and are deeply affected by the development ofprivacy protection in the law. Some defend privacy as focusing oncontrol over information about oneself (Parent, 1983), while othersdefend it as a broader concept required for human dignity (Bloustein,1964), or crucial for intimacy (Gerstein, 1978; Inness, 1992). Othercommentators defend privacy as necessary for the development of variedand meaningful interpersonal relationships (Fried, 1970, Rachels,1975), or as the value that accords us the ability to control theaccess others have to us (Gavison, 1980; Allen, 1988; Moore, 2003), oras a set of norms necessary not only to control access but also toenhance personal expression and choice (Schoeman, 1992), or somecombination of these ({DeCew}, 1997). Discussion of the concept iscomplicated by the fact that privacy appears to be something we valueto provide a sphere within which we can be free from interference byothers, and yet it also appears to function negatively, as the cloakunder which one can hide domination, degradation, or physical harm towomen and others., This essay will discuss all of these topics, namely, (1) thehistorical roots of the concept of privacy, including the developmentof privacy protection in tort and constitutional law, and thephilosophical responses that privacy is merely reducible to otherinterests or is a coherent concept with fundamental value, (2) thecritiques of privacy as a right, (3) the wide array of philosophicaldefinitions or defenses of privacy as a concept, providing alternativeviews on the meaning and value of privacy (and whether or not it isculturally relative), as well as (4) the challenges to privacy posedin an age of technological advance. Overall, most writers defend thevalue of privacy protection despite the difficulties inherent in itsdefinition and its potential use to shield abuse. A contemporarycollection of essays on privacy provides strong evidence to supportthis point (Paul et al., 2000). The contributing authorsexamine various aspects of the right to privacy and its role in moralphilosophy, legal theory, and public policy. They also addressjustifications and foundational arguments for privacy rights.},
	booktitle = {The Stanford Encyclopedia of Philosophy},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {{DeCew}, Judith},
	editor = {Zalta, Edward N.},
	urldate = {2018-12-18},
	date = {2018},
	keywords = {autonomy: in moral and political philosophy, feminist philosophy, interventions: philosophy of law, information technology: and privacy, legal philosophy, legal rights, liberty: positive and negative, rights, rights: human, torts, theories of the common law of}
}

@online{tabarrok_facebook_2018,
	title = {The Facebook Trials: It's Not "Our" Data},
	url = {https://marginalrevolution.com/marginalrevolution/2018/04/facebook-trials-not-data.html},
	shorttitle = {The Facebook Trials},
	abstract = {Facebook, Google and other tech companies are accused of stealing our data or at least of using it without our permission to become extraordinarily rich. Now is the time, say the critics, to stand up and take back our data. Ours, ours, ours. In this way of thinking, our data is like our lawnmower and …},
	titleaddon = {Marginal Revolution},
	author = {Tabarrok, Alex},
	urldate = {2018-12-18},
	date = {2018-04-16},
	langid = {american}
}

@article{kilbertus_avoiding_2017-1,
	title = {Avoiding Discrimination through Causal Reasoning},
	url = {http://arxiv.org/abs/1706.02744},
	abstract = {Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.},
	journaltitle = {{arXiv}:1706.02744 [cs, stat]},
	author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Schölkopf, Bernhard},
	urldate = {2018-12-17},
	date = {2017-06-08},
	eprinttype = {arxiv},
	eprint = {1706.02744},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{sen_race_2016,
	title = {Race as a Bundle of Sticks: Designs that Estimate Effects of Seemingly Immutable Characteristics},
	volume = {19},
	issn = {1094-2939, 1545-1577},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-polisci-032015-010015},
	doi = {10.1146/annurev-polisci-032015-010015},
	shorttitle = {Race as a Bundle of Sticks},
	abstract = {Although understanding the role of race, ethnicity, and identity is central to political science, methodological debates persist about whether it is possible to estimate the effect of something immutable. At the heart of the debate is an older theoretical question: Is race best understood under an essentialist or constructivist framework? In contrast to the “immutable characteristics” or essentialist approach, we argue that race should be operationalized as a “bundle of sticks” that can be disaggregated into elements. With elements of race, causal claims may be possible using two designs: (a) studies that measure the effect of exposure to a racial cue and (b) studies that exploit within-group variation to measure the effect of some manipulable element. These designs can reconcile scholarship on race and causation and offer a clear framework for future research.},
	pages = {499--522},
	number = {1},
	journaltitle = {Annual Review of Political Science},
	author = {Sen, Maya and Wasow, Omar},
	urldate = {2018-12-17},
	date = {2016-05-11},
	langid = {english}
}

@article{celis_ranking_2017,
	title = {Ranking with Fairness Constraints},
	url = {http://arxiv.org/abs/1704.06840},
	abstract = {Ranking algorithms are deployed widely to order a set of items in applications such as search engines, news feeds, and recommendation systems. Recent studies, however, have shown that, left unchecked, the output of ranking algorithms can result in decreased diversity in the type of content presented, promote stereotypes, and polarize opinions. In order to address such issues, we study the following variant of the traditional ranking problem when, in addition, there are fairness or diversity constraints. Given a collection of items along with 1) the value of placing an item in a particular position in the ranking, 2) the collection of sensitive attributes (such as gender, race, political opinion) of each item and 3) a collection of constraints that, for each k, bound the number of items with each attribute that are allowed to appear in the top k positions of the ranking, the goal is to output a ranking that maximizes the value with respect to the original rank quality metric while respecting the constraints. This problem encapsulates various well-studied problems related to bipartite and hypergraph matching as special cases and turns out to be hard to approximate even with simple constraints. Our main technical contributions are fast exact and approximation algorithms along with complementary hardness results that, together, come close to settling the approximability of this constrained ranking maximization problem. Unlike prior work on the constrained matching problems, our algorithm runs in linear time, even when the number of constraints is large, its approximation ratio does not depend on the number of constraints, and it produces solutions with small constraint violations. Our results rely on insights about the constrained matching problem when the objective satisfies properties that appear in common ranking metrics such as Discounted Cumulative Gain, Spearman's rho or Bradley-Terry.},
	journaltitle = {{arXiv}:1704.06840 [cs]},
	author = {Celis, L. Elisa and Straszak, Damian and Vishnoi, Nisheeth K.},
	urldate = {2018-12-15},
	date = {2017-04-22},
	eprinttype = {arxiv},
	eprint = {1704.06840},
	keywords = {Computer Science - Computers and Society, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Technical}
}

@article{jin_what_2018,
	title = {What China Can Gain from Trump’s Trade War},
	url = {https://www.project-syndicate.org/commentary/china-gains-us-trade-war-by-keyu-jin-2018-09},
	abstract = {An unintended outcome of {US} President Donald Trump's trade war is that China will reduce its reliance on foreign trade and imported technologies. The end result could be a China that is stronger, more resilient, and possibly less willing to acquiesce to {US}-designed rules.},
	journaltitle = {Project Syndicate},
	author = {Jin, Keyu},
	urldate = {2018-12-14},
	date = {2018-09-26},
	langid = {english}
}

@article{the_editorial_board_there_2018,
	title = {There May Soon Be Three Internets. America’s Won’t Necessarily Be the Best.},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2018/10/15/opinion/internet-google-china-balkanization.html},
	abstract = {A breakup of the web grants privacy, security and freedom to some, and not so much to others.},
	journaltitle = {The New York Times},
	author = {The Editorial Board},
	urldate = {2018-12-14},
	date = {2018-10-19},
	langid = {american},
	keywords = {Alphabet Inc, Amazon.com Inc, Baidu Inc, Brin, Sergey, Censorship, Computer Network Outages, Computers and the Internet, Google Inc, Schmidt, Eric E, Surveillance of Citizens by Government, Trump, Donald J, United States Politics and Government}
}

@online{richardson_how_2016,
	title = {How to Deal With China’s Human Rights Abuses},
	url = {https://www.hrw.org/news/2016/09/01/how-deal-chinas-human-rights-abuses},
	titleaddon = {Human Rights Watch},
	author = {Richardson, Sophie},
	urldate = {2018-12-13},
	date = {2016-09-01},
	langid = {english}
}

@article{shen_how_2018,
	title = {How does Baidu’s search engine compare to Google?},
	url = {https://www.abacusnews.com/big-guns/how-does-baidus-search-engine-compare-google/article/2158003},
	abstract = {Baidu’s stock drops amid the possibility of competition with Google},
	journaltitle = {Abacus},
	author = {Shen, Xinmei},
	urldate = {2018-12-12},
	date = {2018-08-02}
}

@article{liao_baidu_2018,
	title = {Baidu {CEO} says it will defeat Google if it returns to China},
	url = {https://www.theverge.com/2018/8/7/17660364/baidu-ceo-google-china},
	abstract = {People aren’t so sure},
	journaltitle = {The Verge},
	author = {Liao, Shannon},
	urldate = {2018-12-12},
	date = {2018-08-07}
}

@article{russell_it_2018,
	title = {It turns out some Google staff do believe in controversial plan to re-enter China},
	url = {http://social.techcrunch.com/2018/11/28/google-dragonfly-letter/},
	abstract = {Google’s controversial plan to launch products custom-made for China has been panned by politicians, free speech advocates, ex-staff and many others, but there appears to be some support within Google from employees who actually favor the strategy. Under a project code-named “Dragonfly,…},
	journaltitle = {{TechCrunch}},
	author = {Russell, Jon and Hatmaker, Taylor},
	urldate = {2018-12-14},
	date = {2018-11-28},
	langid = {american}
}

@online{google_employees_against_dragonfly_we_2018,
	title = {We are Google employees. Google must drop Dragonfly.},
	url = {https://medium.com/@googlersagainstdragonfly/we-are-google-employees-google-must-drop-dragonfly-4c8a30c5e5eb},
	abstract = {We are Google employees and we join Amnesty International in calling on Google to cancel project Dragonfly, Google’s effort to create a…},
	titleaddon = {Medium},
	author = {Google Employees Against Dragonfly},
	urldate = {2018-12-13},
	date = {2018-11-27}
}

@article{ward_googles_2010,
	title = {Google's Move to Hong Kong 'A Face-Saving Capitulation'},
	url = {http://www.spiegel.de/international/world/the-world-from-berlin-google-s-move-to-hong-kong-a-face-saving-capitulation-a-685452.html},
	abstract = {Google has stopped playing by the Chinese rules and moved its search engine to Hong Kong. The move allows the company to keep one foot in China while fulfilling its promise to end self-censorship. But German commentators are still skeptical of the company's motives -- and future.},
	journaltitle = {Spiegel Online},
	author = {Ward, Josh},
	urldate = {2018-12-14},
	date = {2010-03-24},
	keywords = {Data Protection, Google, Privacy}
}

@article{gallagher_google_2018,
	title = {Google Plans to Launch Censored Search Engine in China, Leaked Documents Reveal},
	url = {https://theintercept.com/2018/08/01/google-china-search-engine-censorship/},
	journaltitle = {The Intercept},
	author = {Gallagher, Ryan},
	urldate = {2018-12-14},
	date = {2018-08-01},
	langid = {american}
}

@article{gallagher_google_2018-1,
	title = {Google Shut Out Privacy and Security Teams From Secret China Project},
	url = {https://theintercept.com/2018/11/29/google-china-censored-search/},
	journaltitle = {The Intercept},
	author = {Gallagher, Ryan},
	urldate = {2018-11-30},
	date = {2018-11-29},
	langid = {american}
}

@online{grafton_donald_2009,
	title = {Donald Kagan's Thucydides: The Reinvention of History.},
	url = {https://slate.com/culture/2009/10/donald-kagan-s-thucydides-the-reinvention-of-history.html},
	shorttitle = {Donald Kagan's Thucydides},
	abstract = {Modern readers are often shocked to learn that the Athenians—citizens of a free city who defeated the Persians when they invaded Greece, built the...},
	titleaddon = {Slate Magazine},
	author = {Grafton, Anthony},
	urldate = {2018-12-14},
	date = {2009-10-19},
	langid = {english}
}

@online{vigo_project_2018,
	title = {Project Dragonfly And Google's Threat To Anti-Democratic Processes},
	url = {https://www.forbes.com/sites/julianvigo/2018/10/18/project-dragonfly-and-googles-threat-to-anti-democratic-processes/},
	abstract = {It was revealed in August that Google was working with the Chinese government to create censored search in the country.  What are the dangers of a private company collaborating to undermine democratic institutions such as the freedom of expression and conscience abroad?},
	titleaddon = {Forbes},
	author = {Vigo, Julian},
	urldate = {2018-12-14},
	date = {2018-10-28},
	langid = {english}
}

@article{raz_human_2010,
	title = {Human Rights in the Emerging World Order},
	volume = {1},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=1497055},
	doi = {10.2139/ssrn.1497055},
	abstract = {This article has been published by the journal Transnational Legal Theory (http://www.hartjournals.co.uk/{TLT}). The article is an expanded and revised version of the lecture I gave at the opening plenary session of the 24th {IVR} World Congress in Beijing, September 2009, which was entitled and previously uploaded as ‘Human Rights in a New World Order.’ The unrevised ‘Human Rights in a New World Order’ speech will appear in the {IVR} proceedings as well as in translation in Chinese. The present article is made available for download here immediately after publication by special arrangement with the journal. The article is a reflection on the importance and some of the problems involved with the practice of human rights in international relations in the age of globalization. Beginning with rights in general to claim that their justification is in protecting and advancing individual interest, and distributing power to individuals. This is the main distinctive contribution of human rights in the international arena: they empower individuals, and voluntary organizations, endowing them with a voice alongside states and multinational corporations, and creating an additional channel of political action. I argue that human rights recognized in human rights law and practice are not universal rights, but they are syncronically universal, pertaining to all human beings alive today. I explain and justify that feature by the fact that human rights set a limit to state sovereignty. This fact makes clear the importance of impartial, efficient and reliable institutions for administering and enforcing human rights. Where such institutions are impossible there are no human right. Even when they are possible we face the risk that the practice of human rights would lead to an international regime which is blind to cultural diversity, and tends to serve the interests of big businesses and nothing more. This - I claim - is not something inseparable from the idea of human rights, but it confronts its practice with as yet unresolved problems. The human rights to education and to health are used to illustrate the points made in the paper.},
	pages = {31--47},
	number = {1},
	journaltitle = {Transnational Legal Theory},
	author = {Raz, Joseph},
	urldate = {2018-12-14},
	date = {2010},
	langid = {english}
}

@online{richardson_is_2018,
	title = {Is China winning its fight against rights at the {UN}?},
	url = {https://www.hrw.org/news/2018/12/12/china-winning-its-fight-against-rights-un},
	abstract = {Few governments work harder to eliminate criticism of their human rights record at the United Nations than China. And China’s efforts may well make it easier for other abusive actors to escape scrutiny. Other governments — and the U.N. system itself — should push back against these encroachments.},
	titleaddon = {Human Rights Watch},
	author = {Richardson, Sophie},
	urldate = {2018-12-13},
	date = {2018-12-12},
	langid = {english}
}

@report{bader_u.s.-china_2018,
	title = {U.S.-China relations: Is it time to end the engagement?},
	url = {https://www.brookings.edu/research/u-s-china-relations-is-it-time-to-end-the-engagement/},
	abstract = {Intensive engagement with China, which has been a foundation of U.S. policy since President Richard Nixon’s 1972 visit, is under attack by critics of U.S. policy who seek to disengage the two countries. China’s growing strength, its perceived challenge to U.S. global leadership, its economic mercantilism, and other actions that are seen as threatening have persuaded many American policymakers and analysts that engagement no longer serves U.S. interest as we head into a period of intense rivalry. The Trump administration, legislators, and corporate and academic institutions are in the process of abandoning long-standing cooperative arrangements and programs affecting trade, investment, students on American campuses, funding of academic programs, media, and military interaction.

Engagement was never undertaken as a favor to China but because it was judged to be in the U.S. interest. Its abandonment would be highly likely to exacerbate hostility between the United States and China, persuade Chinese leaders and citizens alike that a more adversarial stance toward the United States is necessary, and advantage other countries operating in China that will not follow the U.S. path of disengagement. Continuing intensive engagement in no way would prevent alterations in U.S. policy to respond to challenges from China in the economic, digital, academic, and security fields. Indeed it would likely make policy changes more effective by giving China a continuing stake in the relationship with the United States.},
	pages = {7},
	institution = {The Brookings Institution},
	author = {Bader, Jeffrey A.},
	date = {2018-09},
	langid = {english}
}

@online{piccone_chinas_2018,
	title = {China’s long game on human rights},
	url = {https://www.brookings.edu/blog/order-from-chaos/2018/09/24/chinas-long-game-on-human-rights/},
	abstract = {So far, China’s more assertive gambit on human rights has not always won the day. Despite the U.S. withdrawal, and China’s growing efforts to sway others, a core bloc of democratic states remain steadfastly committed to bolstering the international human rights regime. Leading European actors like France, Germany, Sweden, and the Netherlands, joined by Japan, Canada, South Korea, and Australia, are building coalitions with states like Brazil, Mexico, Argentina, Tunisia, Ghana, Georgia, and Ukraine to hold the line on key principles. The U.S. Congress can do its part by ensuring that the system’s building blocks are properly funded. And the new High Commissioner for Human Rights, Michelle Bachelet, can speak with moral clarity for defending the principles that China, slowly but surely, now seeks to undermine.},
	titleaddon = {The Brookings Institution},
	author = {Piccone, Ted},
	urldate = {2018-12-13},
	date = {2018-09-24}
}

@article{martin_how_2018,
	title = {How Chinese students exercise free speech abroad},
	issn = {0013-0613},
	url = {https://www.economist.com/open-future/2018/06/11/how-chinese-students-exercise-free-speech-abroad},
	abstract = {Students aren’t stooges for the government, says Fran Martin of the University of Melbourne},
	journaltitle = {The Economist},
	author = {Martin, Fran},
	urldate = {2018-12-13},
	date = {2018-06-11}
}

@online{human_rights_watch_open_2018,
	title = {Open Letter to Google on Reported Plans to Launch a Censored Search Engine in China},
	url = {https://www.hrw.org/news/2018/08/28/open-letter-google-reported-plans-launch-censored-search-engine-china},
	abstract = {Like many of Google’s own employees, we are extremely concerned by reports that Google is developing a new censored search engine app for the Chinese market. The project, codenamed “Dragonfly”, would represent an alarming capitulation by Google on human rights.  The Chinese government extensively violates the rights to freedom of expression and privacy; by accommodating the Chinese authorities’ repression of dissent, Google would be actively participating in those violations for millions of internet users in China.},
	titleaddon = {Human Rights Watch},
	author = {Human Rights Watch},
	urldate = {2018-12-13},
	date = {2018-08-28},
	langid = {english}
}

@article{helft_google_2010,
	title = {Google to Redirect China Users to Uncensored Site},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2010/03/23/technology/23google.html},
	abstract = {Google said it would direct users to an uncensored version based in Hong Kong. China on Tuesday said the case would not affect ties with the U.S. “unless politicized” by others.},
	journaltitle = {The New York Times},
	author = {Helft, Miguel and Barboza, David},
	urldate = {2018-12-12},
	date = {2010-03-22},
	langid = {american},
	keywords = {Censorship, China, Google Inc, Hong Kong, Relocation of Business, Search Engines}
}

@article{mozur_reach_2017,
	title = {To Reach China, {LinkedIn} Plays by Local Rules},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2014/10/06/technology/to-reach-china-linkedin-plays-by-local-rules.html},
	abstract = {The professional social network’s Chinese-language version, which lacks certain features of Western versions, seems to have the tacit approval of China’s government.},
	journaltitle = {The New York Times},
	author = {Mozur, Paul and Goel, Vindu},
	urldate = {2018-12-12},
	date = {2017-12-21},
	langid = {american},
	keywords = {Censorship, China, Chinese Language, Computers and the Internet, Goel, Vindu, {LinkedIn} Corporation, Mozur, Paul, Social Media}
}

@article{yuan_google_2018,
	title = {Google, Seeking a Return to China, Is Said to Be Building a Censored Search Engine},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2018/08/01/technology/china-google-censored-search-engine.html},
	abstract = {Google withdrew from China in 2010 to protest the country’s censorship. Now the internet giant is working on a search engine that complies with Chinese censorship rules.},
	journaltitle = {The New York Times},
	author = {Yuan, Li and Wakabayashi, Daisuke},
	urldate = {2018-12-12},
	date = {2018-08-07},
	langid = {american},
	keywords = {Censorship, China, Freedom of Speech and Expression, Google Inc, Human Rights and Human Rights Violations, Politics and Government, Search Engines}
}

@online{yang_beijing_2018,
	title = {Beijing mouthpiece tells Google 'welcome back, now obey the rules'},
	url = {https://www.scmp.com/tech/article/2158591/peoples-daily-posts-commentary-welcoming-google-back-china-stipulates-it-must},
	abstract = {The commentary states that Google’s decision to exit the Chinese market was a “huge blunder”, which resulted in the company missing “golden chances”},
	titleaddon = {South China Morning Post},
	author = {Yang, Yingzhi},
	urldate = {2018-12-12},
	date = {2018-08-08},
	langid = {english}
}

@online{horwitz_why_2018,
	title = {Why internet users chose Baidu over Google when it was in China},
	url = {https://qz.com/1352137/why-internet-users-chose-baidu-over-google-when-it-was-in-china/},
	abstract = {It wasn't just politics. Strategy helped the local search giant win.},
	titleaddon = {Quartz},
	author = {Horwitz, Josh},
	urldate = {2018-12-12},
	date = {2018-08-09},
	langid = {english}
}
@online{horwitz_google_2018,
	title = {Google search might return to China’s internet, and some users can hardly wait},
	url = {https://qz.com/1346580/google-search-might-return-to-chinas-internet-and-some-users-can-hardly-wait/},
	abstract = {Baidu, the country's dominant search provider, is less enthusiastic.},
	titleaddon = {Quartz},
	author = {Horwitz, Josh},
	urldate = {2018-12-12},
	date = {2018-08-02},
	langid = {english}
}

@online{horwitz_chinese_2018,
	title = {Chinese search giant Baidu warns Google "the world is now copying from China"},
	url = {https://qz.com/1351130/baidu-wants-google-to-know-the-competition-in-china-is-going-to-be-tougher-than-ever/},
	abstract = {"If Google decides to return to China, we have enough confidence to take them on once more, and win once more."},
	titleaddon = {Quartz},
	author = {Horwitz, Josh},
	urldate = {2018-12-12},
	date = {2018-08-08},
	langid = {english}
}

@online{horwitz_apples_2018,
	title = {Apple's {iCloud} service in China will be managed by a data firm started by the government},
	url = {https://qz.com/1176376/apples-icloud-service-in-china-will-be-managed-by-a-data-firm-started-by-the-government/},
	abstract = {The hardware company will transfer data from its Chinese users to a local data center.},
	titleaddon = {Quartz},
	author = {Horwitz, Josh},
	urldate = {2018-12-12},
	date = {2018-01-10},
	langid = {english}
}

@online{horwitz_google-tencent_2018,
	title = {A Google-Tencent alliance makes for murky politics—but great business},
	url = {https://qz.com/1350165/a-google-tencent-alliance-in-china-makes-for-murky-politics-but-great-business/},
	abstract = {The search giant and social giant look set to be teaming up. And politics aside, the two complement one another.},
	titleaddon = {Quartz},
	author = {Horwitz, Josh},
	urldate = {2018-12-12},
	date = {2018-08-07},
	langid = {english}
}

@article{cadell_google_2018,
	title = {Google plans return to China search market with censored app: sources},
	url = {https://www.reuters.com/article/us-china-google-idUSKBN1KN09C},
	shorttitle = {Google plans return to China search market with censored app},
	abstract = {Alphabet Inc’s Google plans to launch a version of its search engine in China that will block some websites and search terms, two sources said, in a move that could mark its return to a market it abandoned eight years ago on censorship concerns.},
	journaltitle = {Reuters},
	author = {Cadell, Cate and Menn, Joseph},
	urldate = {2018-12-12},
	date = {2018-08-02},
	langid = {english},
	keywords = {Asia / Pacific, {CHINA}, China ({PRC}), Company News, Computer and Electronics Retailers ({TRBC}), Emerging Market Countries, Freedom of Speech / Censorship, {GOOGLE}, General News, Hong Kong, {IT} Services and Consulting ({TRBC}), Internet / World Wide Web, Major News, Pictures, Regulation, Society / Social Issues, Software ({TRBC}), Software and {IT} Services ({TRBC}), Technology ({TRBC}), Technology / Media / Telecoms, Telecommunications Services ({TRBC}), {US}, United States}
}

@online{fang_eight_2018,
	title = {Eight years later, Google Search won't beat Baidu},
	url = {https://technode.com/2018/08/16/google-vs-baidu/},
	abstract = {There is little chance that Google’s return will change the established scene in China’s search engine sector.},
	titleaddon = {{TechNode}},
	author = {Fang, Tianyu},
	urldate = {2018-12-12},
	date = {2018-08-16},
	langid = {american}
}

@article{king_reverse-engineering_2014,
	title = {Reverse-engineering censorship in China: Randomized experimentation and participant observation},
	volume = {345},
	rights = {Copyright © 2014, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/345/6199/1251722},
	doi = {10.1126/science.1251722},
	shorttitle = {Reverse-engineering censorship in China},
	abstract = {Structured Abstract
Introduction Censorship has a long history in China, extending from the efforts of Emperor Qin to burn Confucian texts in the third century {BCE} to the control of traditional broadcast media under Communist Party rule. However, with the rise of the Internet and new media platforms, more than 1.3 billion people can now broadcast their individual views, making information far more diffuse and considerably harder to control. In response, the government has built a massive social media censorship organization, the result of which constitutes the largest selective suppression of human communication in the recorded history of any country. We show that this large system, designed to suppress information, paradoxically leaves large footprints and so reveals a great deal about itself and the intentions of the government.
{\textless}img class="highwire-embed" alt="Embedded Image" src="http://science.sciencemag.org/sites/default/files/highwire/sci/345/6199/1251722/embed/inline-graphic-1.gif"/{\textgreater}The Chinese censorship decision tree. The pictures shown are examples of real (and typical) websites, along with our translations.
Rationale Chinese censorship of individual social media posts occurs at two levels: (i) Many tens of thousands of censors, working inside Chinese social media firms and government at several levels, read individual social media posts, and decide which ones to take down. (ii) They also read social media submissions that are prevented from being posted by automated keyword filters, and decide which ones to publish. To study the first level, we devised an observational study to download published Chinese social media posts before the government could censor them, and to revisit each from a worldwide network of computers to see which was censored. To study the second level, we conducted the first largescale experimental study of censorship by creating accounts on numerous social media sites throughout China, submitting texts with different randomly assigned content to each, and detecting from a worldwide network of computers which ones were censored. To find out the details of how the system works, we supplemented the typical current approach (conducting uncertain and potentially unsafe confidential interviews with insiders) with a participant observation study, in which we set up our own social media site in China. While also attempting not to alter the system we were studying, we purchased a {URL}, rented server space, contracted with Chinese firms to acquire the same software as used by existing social media sites, and—with direct access to their software, documentation, and even customer service help desk support—reverseengineered how it all works.
Results Criticisms of the state, its leaders, and their policies are routinely published, whereas posts with collective action potential are much more likely to be censored—regardless of whether they are for or against the state (two concepts not previously distinguished in the literature). Chinese people can write the most vitriolic blog posts about even the top Chinese leaders without fear of censorship, but if they write in support of or opposition to an ongoing protest—or even about a rally in favor of a popular policy or leader—they will be censored. We clarify the internal mechanisms of the Chinese censorship apparatus and show how changes in censorship behavior reveal government intent by presaging their action on the ground. That is, it appears that criticism on the web, which was thought to be censored, is used by Chinese leaders to determine which officials are not doing their job of mollifying the people and need to be replaced.
Conclusion Censorship in China is used to muzzle those outside government who attempt to spur the creation of crowds for any reason—in opposition to, in support of, or unrelated to the government. The government allows the Chinese people to say whatever they like about the state, its leaders, or their policies, because talk about any subject unconnected to collective action is not censored. The value that Chinese leaders find in allowing and then measuring criticism by hundreds of millions of Chinese people creates actionable information for them and, as a result, also for academic scholars and public policy analysts.
Censorship of social media in China
Figuring out how many and which social media comments are censored by governments is difficult because those comments, by definition, cannot be read. King et al. have posted comments to social media sites in China and then waited to see which of these never appeared, which appeared and were then removed, and which appeared and survived. About 40\% of their submissions were reviewed by an army of censors, and more than half of these never appeared. By varying the content of posts across topics, they conclude that any mention of collective action is selectively suppressed.
Science, this issue 10.1126/science.1251722
Existing research on the extensive Chinese censorship organization uses observational methods with well-known limitations. We conducted the first large-scale experimental study of censorship by creating accounts on numerous social media sites, randomly submitting different texts, and observing from a worldwide network of computers which texts were censored and which were not. We also supplemented interviews with confidential sources by creating our own social media site, contracting with Chinese firms to install the same censoring technologies as existing sites, and—with their software, documentation, and even customer support—reverse-engineering how it all works. Our results offer rigorous support for the recent hypothesis that criticisms of the state, its leaders, and their policies are published, whereas posts about real-world events with collective action potential are censored.
China censors online posts that advocate collective action.
China censors online posts that advocate collective action.},
	pages = {1251722},
	number = {6199},
	journaltitle = {Science},
	author = {King, Gary and Pan, Jennifer and Roberts, Margaret E.},
	urldate = {2018-12-12},
	date = {2014-08-22},
	langid = {english},
	pmid = {25146296}
}

@article{king_how_2013,
	title = {How Censorship in China Allows Government Criticism but Silences Collective Expression},
	volume = {107},
	abstract = {We offer the first large scale, multiple source analysis of the outcome of what may be the most extensive effort to selectively censor human expression ever implemented. To do this, we have devised a system to locate, download, and analyze the content of millions of social media posts originating from nearly 1,400 different social media services all over China before the Chinese government is able to find, evaluate, and censor (i.e., remove from the Internet) the large subset they deem objectionable. Using modern computer-assisted text analytic methods that we adapt to and validate in the Chinese language, we compare the substantive content of posts censored to those not censored over time in each of 85 topic areas. Contrary to previous understandings, posts with negative, even vitriolic, criticism of the state, its leaders, and its policies are not more likely to be censored. Instead, we show that the censorship program is aimed at curtailing collective action by silencing comments that represent, reinforce, or spur social mobilization, regardless of content. Censorship is oriented toward attempting to forestall collective activities that are occurring now or may occur in the future --- and, as such, seem to clearly expose government intent.},
	pages = {1--18},
	number = {2},
	journaltitle = {American Political Science Review},
	author = {King, Gary and Pan, Jennifer and Roberts, Margaret E.},
	date = {2013},
	note = {Please see our followup article published in Science, “Reverse-Engineering Censorship In China: Randomized Experimentation And Participant Observation.”}
}

@online{brandom_congress_2018,
	title = {Congress thinks Google has a bias problem — does it?},
	url = {https://www.theverge.com/2018/12/12/18136619/google-bias-sundar-pichai-google-hearing},
	abstract = {Google’s bias is more complicated than left or right},
	titleaddon = {The Verge},
	author = {Brandom, Russell},
	urldate = {2018-12-12},
	date = {2018-12-12}
}

@online{reporters_without_borders_open_2018,
	title = {Open letter: Over 50 {NGOs} urge Google not to aid China’s censorship and surveillance regime},
	url = {https://www.hongkongfp.com/2018/12/11/open-letter-50-ngos-urge-google-not-aid-chinas-censorship-surveillance-regime/},
	shorttitle = {Open letter},
	abstract = {The Intercept revealed in August that Google is planning to develop a censored version of its search engine in China. The project, codenamed “Dragonfly,” has been criticised by employees and human rights activists as an attack on internet freedom. The following is an open letter addressed to Sundar Pichai, Chief Executive Officer of Google Inc., published on …},
	titleaddon = {Hong Kong Free Press {HKFP}},
	author = {Reporters Without Borders},
	urldate = {2018-12-12},
	date = {2018-12-11},
	langid = {british}
}

@online{lecher_what_2018,
	title = {What Google’s {CEO} told Congress about China — and what he didn’t},
	url = {https://www.theverge.com/2018/12/12/18136544/sundar-pichai-google-house-hearing-china-dragonly},
	abstract = {Pichai says there are no plans to launch search "right now."},
	titleaddon = {The Verge},
	author = {Lecher, Colin},
	urldate = {2018-12-12},
	date = {2018-12-12}
}

@article{hannart_causal_2016,
	title = {Causal Counterfactual Theory for the Attribution of Weather and Climate-Related Events},
	volume = {97},
	issn = {0003-0007, 1520-0477},
	url = {http://journals.ametsoc.org/doi/10.1175/BAMS-D-14-00034.1},
	doi = {10.1175/BAMS-D-14-00034.1},
	pages = {99--110},
	number = {1},
	journaltitle = {Bulletin of the American Meteorological Society},
	author = {Hannart, A. and Pearl, J. and Otto, F. E. L. and Naveau, P. and Ghil, M.},
	urldate = {2018-12-09},
	date = {2016-01},
	langid = {english}
}

@article{hannart_probabilities_2018,
	title = {Probabilities of causation of climate changes},
	volume = {31},
	issn = {0894-8755, 1520-0442},
	url = {http://arxiv.org/abs/1712.00063},
	doi = {10.1175/JCLI-D-17-0304.1},
	abstract = {Multiple changes in Earth's climate system have been observed over the past decades. Determining how likely each of these changes are to have been caused by human influence, is important for decision making on mitigation and adaptation policy. Here we describe an approach for deriving the probability that anthropogenic forcings have caused a given observed change. The proposed approach is anchored into causal counterfactual theory (Pearl 2009) which has been introduced recently, and was in fact partly used already, in the context of extreme weather event attribution ({EA}). We argue that these concepts are also relevant, and can be straightforwardly extended to, the context of detection and attribution of long term trends associated to climate change (D\&A). For this purpose, and in agreement with the principle of "fingerprinting" applied in the conventional D\&A framework, a trajectory of change is converted into an event occurrence defined by maximizing the causal evidence associated to the forcing under scrutiny. Other key assumptions used in the conventional D\&A framework, in particular those related to numerical models error, can also be adapted conveniently to this approach. Our proposal thus allows to bridge the conventional framework with the standard causal theory, in an attempt to improve the quantification of causal probabilities. An illustration suggests that our approach is prone to yield a significantly higher estimate of the probability that anthropogenic forcings have caused the observed temperature change, thus supporting more assertive causal claims.},
	pages = {5507--5524},
	number = {14},
	journaltitle = {Journal of Climate},
	author = {Hannart, Alexis and Naveau, Philippe},
	urldate = {2018-12-08},
	date = {2018-07},
	eprinttype = {arxiv},
	eprint = {1712.00063},
	keywords = {Statistics - Applications}
}

@article{putnam_e_2007,
	title = {E Pluribus Unum: Diversity and Community in the Twenty-first Century (The 2006 Johan Skytte Prize Lecture)},
	volume = {30},
	issn = {0080-6757, 1467-9477},
	url = {http://doi.wiley.com/10.1111/j.1467-9477.2007.00176.x},
	doi = {10.1111/j.1467-9477.2007.00176.x},
	shorttitle = {E Pluribus Unum},
	pages = {137--174},
	number = {2},
	journaltitle = {Scandinavian Political Studies},
	author = {Putnam, Robert D.},
	urldate = {2018-12-08},
	date = {2007-06},
	langid = {english}
}

@article{king_making_2000,
	title = {Making the Most of Statistical Analyses: Improving Interpretation and Presentation},
	volume = {44},
	issn = {00925853},
	url = {https://www.jstor.org/stable/2669316?origin=crossref},
	doi = {10.2307/2669316},
	shorttitle = {Making the Most of Statistical Analyses},
	pages = {347},
	number = {2},
	journaltitle = {American Journal of Political Science},
	author = {King, Gary and Tomz, Michael and Wittenberg, Jason},
	urldate = {2018-12-08},
	date = {2000-04}
}

@thesis{griths_causes_2004,
	title = {Causes, Coincidences, and Theories},
	url = {http://cocosci.berkeley.edu/tom/papers/tomthesis.pdf},
	pagetotal = {254},
	institution = {Stanford University},
	type = {Doctoral dissertation},
	author = {Griﬃths, Thomas L.},
	date = {2004},
	langid = {english}
}

@article{goudet_learning_2018,
	title = {Learning Functional Causal Models with Generative Neural Networks},
	url = {http://arxiv.org/abs/1709.05321},
	doi = {10.1007/978-3-319-98131-4},
	abstract = {We introduce a new approach to functional causal modeling from observational data, called Causal Generative Neural Networks ({CGNN}). {CGNN} leverages the power of neural networks to learn a generative model of the joint distribution of the observed variables, by minimizing the Maximum Mean Discrepancy between generated and observed data. An approximate learning criterion is proposed to scale the computational cost of the approach to linear complexity in the number of observations. The performance of {CGNN} is studied throughout three experiments. Firstly, {CGNN} is applied to cause-effect inference, where the task is to identify the best causal hypothesis out of \$X{\textbackslash}rightarrow Y\$ and \$Y{\textbackslash}rightarrow X\$. Secondly, {CGNN} is applied to the problem of identifying v-structures and conditional independences. Thirdly, {CGNN} is applied to multivariate functional causal modeling: given a skeleton describing the direct dependences in a set of random variables \${\textbackslash}textbf\{X\} = [X\_1, {\textbackslash}ldots, X\_d]\$, {CGNN} orients the edges in the skeleton to uncover the directed acyclic causal graph describing the causal structure of the random variables. On all three tasks, {CGNN} is extensively assessed on both artificial and real-world data, comparing favorably to the state-of-the-art. Finally, {CGNN} is extended to handle the case of confounders, where latent variables are involved in the overall causal model.},
	journaltitle = {{arXiv}:1709.05321 [stat]},
	author = {Goudet, Olivier and Kalainathan, Diviyan and Caillou, Philippe and Guyon, Isabelle and Lopez-Paz, David and Sebag, Michèle},
	urldate = {2018-12-08},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1709.05321},
	keywords = {Statistics - Machine Learning}
}

@article{pearl_theoretical_2018,
	title = {Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution},
	url = {https://arxiv.org/abs/1801.04016},
	author = {Pearl, Judea},
	urldate = {2018-12-08},
	date = {2018-01-11},
	langid = {english}
}

@article{lipton_does_2017,
	title = {Does mitigating {ML}'s impact disparity require treatment disparity?},
	url = {http://arxiv.org/abs/1711.07076},
	abstract = {Following related work in law and policy, two notions of disparity have come to shape the study of fairness in algorithmic decision-making. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups, even if the correlation arises unintentionally. Naturally, we can achieve impact parity through purposeful treatment disparity. In one thread of technical work, papers aim to reconcile the two forms of parity proposing disparate learning processes ({DLPs}). Here, the learning algorithm can see group membership during training but produce a classifier that is group-blind at test time. In this paper, we show theoretically that: (i) When other features correlate to group membership, {DLPs} will (indirectly) implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) When group membership is partly revealed by other features, {DLPs} induce within-class discrimination; and (iii) In general, {DLPs} provide a suboptimal trade-off between accuracy and impact parity. Based on our technical analysis, we argue that transparent treatment disparity is preferable to occluded methods for achieving impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying {DLPs} vs. per-group thresholds.},
	journaltitle = {{arXiv}:1711.07076 [cs, stat]},
	author = {Lipton, Zachary C. and Chouldechova, Alexandra and {McAuley}, Julian},
	urldate = {2018-12-08},
	date = {2017-11-19},
	eprinttype = {arxiv},
	eprint = {1711.07076},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@book{pearl_causality:_2009,
	edition = {2nd},
	title = {Causality: Models, Reasoning, and Inference},
	isbn = {978-0-521-89560-6 978-0-521-77362-1},
	shorttitle = {Causality},
	pagetotal = {384},
	publisher = {Cambridge University Press},
	author = {Pearl, Judea},
	date = {2009},
	keywords = {Causation, Probabilities}
}

@article{garnelo_towards_2016,
	title = {Towards Deep Symbolic Reinforcement Learning},
	url = {http://arxiv.org/abs/1609.05518},
	abstract = {Deep reinforcement learning ({DRL}) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary {DRL} systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural {DRL} system on a stochastic variant of the game.},
	journaltitle = {{arXiv}:1609.05518 [cs]},
	author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
	urldate = {2018-12-02},
	date = {2016-09-18},
	eprinttype = {arxiv},
	eprint = {1609.05518},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@online{weidman_4_2017,
	title = {The 4 Deep Learning Breakthroughs You Should Know About},
	url = {https://towardsdatascience.com/the-5-deep-learning-breakthroughs-you-should-know-about-df27674ccdf2},
	abstract = {The First in a Series on Deep Learning for Non-Experts},
	titleaddon = {Towards Data Science},
	author = {Weidman, Seth},
	urldate = {2018-12-02},
	date = {2017-12-04}
}

@book{waldmann_acquisition_2017,
	title = {The Acquisition and Use of Causal Structure Knowledge},
	volume = {1},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199399550.001.0001/oxfordhb-9780199399550-e-10},
	publisher = {Oxford University Press},
	author = {Rottman, Benjamin Margolin},
	editor = {Waldmann, Michael R.},
	urldate = {2018-12-02},
	date = {2017-05-10},
	langid = {english},
	doi = {10.1093/oxfordhb/9780199399550.013.10}
}

@online{evans_learning_2018,
	title = {Learning explanatory rules from noisy data},
	url = {https://deepmind.com/blog/learning-explanatory-rules-noisy-data/},
	abstract = {Our new paper, recently published in {JAIR}, demonstrates it is possible for systems to combine intuitive perceptual with conceptual interpretable reasoning. The system we describe, ∂{ILP}, is robust to noise, data-efficient, and produces interpretable rules.},
	titleaddon = {{DeepMind}},
	author = {Evans, Richard and Grefenstette, Edward},
	urldate = {2018-12-02},
	date = {2018-01-29}
}

@article{evans_learning_2017,
	title = {Learning Explanatory Rules from Noisy Data},
	url = {http://arxiv.org/abs/1711.04574},
	abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional {ILP} systems are suited for, but shows a robustness to noise and error in the training data which {ILP} cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which {ILP} cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
	journaltitle = {{arXiv}:1711.04574 [cs, math]},
	author = {Evans, Richard and Grefenstette, Edward},
	urldate = {2018-12-02},
	date = {2017-11-13},
	eprinttype = {arxiv},
	eprint = {1711.04574},
	keywords = {Computer Science - Neural and Evolutionary Computing, Mathematics - Logic}
}

@article{chang_automatically_2018,
	title = {Automatically Composing Representation Transformations as a Means for Generalization},
	url = {http://arxiv.org/abs/1807.04640},
	abstract = {How can we build a learner that can capture the essence of what makes a hard problem more complex than a simple one, break the hard problem along characteristic lines into smaller problems it knows how to solve, and sequentially solve the smaller problems until the larger one is solved? To work towards this goal, we focus on learning to generalize in a particular family of problems that exhibit compositional and recursive structure: their solutions can be found by composing in sequence a set of reusable partial solutions. Our key idea is to recast the problem of generalization as a problem of learning algorithmic procedures: we can formulate a solution to this family as a sequential decision-making process over transformations between representations. Our formulation enables the learner to learn the structure and parameters of its own computation graph with sparse supervision, make analogies between problems by transforming one problem representation to another, and exploit modularity and reuse to scale to problems of varying complexity. Experiments on solving a variety of multilingual arithmetic problems demonstrate that our method discovers the hierarchical decomposition of a problem into its subproblems, generalizes out of distribution to unseen problem classes, and extrapolates to harder versions of the same problem, yielding a 10-fold reduction in sample complexity compared to a monolithic recurrent neural network.},
	journaltitle = {{arXiv}:1807.04640 [cs, stat]},
	author = {Chang, Michael B. and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L.},
	urldate = {2018-12-02},
	date = {2018-07-12},
	eprinttype = {arxiv},
	eprint = {1807.04640},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X16001837/type/journal_article},
	doi = {10.1017/S0140525X16001837},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	urldate = {2018-12-02},
	date = {2017},
	langid = {english}
}

@article{muller_fanning_2018,
	title = {Fanning the Flames of Hate: Social Media and Hate Crime},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3082972},
	doi = {10.2139/ssrn.3082972},
	shorttitle = {Fanning the Flames of Hate},
	journaltitle = {{SSRN} Electronic Journal},
	author = {Müller, Karsten and Schwarz, Carlo},
	urldate = {2018-11-09},
	date = {2018},
	langid = {english}
}

@inproceedings{pedreschi_discrimination-aware_2008,
	location = {Las Vegas, Nevada, {USA}},
	title = {Discrimination-aware Data Mining},
	isbn = {978-1-60558-193-4},
	url = {http://dl.acm.org/citation.cfm?doid=1401890.1401959},
	doi = {10.1145/1401890.1401959},
	abstract = {In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on membership to a category or a minority, without regard to individual merit. Rules extracted from databases by data mining techniques, such as classiﬁcation or association rules, when used for decision tasks such as beneﬁt or credit approval, can be discriminatory in the above sense. In this paper, the notion of discriminatory classiﬁcation rules is introduced and studied. Providing a guarantee of non-discrimination is shown to be a non trivial task. A na¨ıve approach, like taking away all discriminatory attributes, is shown to be not enough when other background knowledge is available. Our approach leads to a precise formulation of the redlining problem along with a formal result relating discriminatory rules with apparently safe ones by means of background knowledge. An empirical assessment of the results on the German credit dataset is also provided.},
	eventtitle = {The 14th {ACM} {SIGKDD} international conference},
	pages = {560},
	booktitle = {Proceeding of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} 08},
	publisher = {{ACM} Press},
	author = {Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
	urldate = {2018-09-28},
	date = {2008-08},
	langid = {english},
	keywords = {Technical}
}

@book{weisburd_proactive_2018,
	location = {Washington, D.C.},
	title = {Proactive Policing: Effects on Crime and Communities},
	isbn = {978-0-309-46713-1},
	url = {https://www.nap.edu/catalog/24928},
	shorttitle = {Proactive Policing},
	publisher = {National Academies Press},
	author = {{Committee on Proactive Policing: Effects on Crime, Communities, and Civil Liberties} and {Committee on Law and Justice} and {Division of Behavioral and Social Sciences and Education} and {National Academies of Sciences, Engineering, and Medicine}},
	editor = {Weisburd, David and Majimundar, Malay K.},
	urldate = {2018-09-27},
	date = {2018-02-23},
	doi = {10.17226/24928},
	keywords = {{NonTechnical}}
}

@online{wells_outline_1920,
	title = {The Outline of History},
	url = {http://www.ibiblio.org/pub/docs/books/sherwood/Wells-Outline/Outline_of_History.htm},
	author = {Wells, Herbert George},
	urldate = {2017-02-19},
	date = {1920}
}

@article{reinhart_review_2018,
	title = {A Review of Self-Exciting Spatio-Temporal Point Processes and Their Applications},
	volume = {33},
	issn = {0883-4237},
	url = {http://arxiv.org/abs/1708.02647},
	doi = {10.1214/17-STS629},
	abstract = {Self-exciting spatio-temporal point process models predict the rate of events as a function of space, time, and the previous history of events. These models naturally capture triggering and clustering behavior, and have been widely used in fields where spatio-temporal clustering of events is observed, such as earthquake modeling, infectious disease, and crime. In the past several decades, advances have been made in estimation, inference, simulation, and diagnostic tools for self-exciting point process models. In this review, I describe the basic theory, survey related estimation and inference techniques from each field, highlight several key applications, and suggest directions for future research.},
	pages = {299--318},
	number = {3},
	journaltitle = {Statistical Science},
	author = {Reinhart, Alex},
	urldate = {2018-09-27},
	date = {2018-08},
	eprinttype = {arxiv},
	eprint = {1708.02647},
	keywords = {Statistics - Methodology, Technical}
}

@article{simoiu_problem_2017,
	title = {The problem of infra-marginality in outcome tests for discrimination},
	volume = {11},
	issn = {1932-6157},
	url = {https://projecteuclid.org/euclid.aoas/1507168827},
	doi = {10.1214/17-AOAS1058},
	pages = {1193--1216},
	number = {3},
	journaltitle = {The Annals of Applied Statistics},
	author = {Simoiu, Camelia and Corbett-Davies, Sam and Goel, Sharad},
	urldate = {2018-06-25},
	date = {2017-09},
	langid = {english},
	keywords = {Technical}
}

@book{pearl_causal_2016,
	location = {Chichester, West Sussex},
	title = {Causal Inference in Statistics: A Primer},
	isbn = {978-1-119-18684-7},
	shorttitle = {Causal inference in statistics},
	pagetotal = {136},
	publisher = {Wiley},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	date = {2016},
	keywords = {Causation, Mathematical statistics, Probabilities, Technical}
}

@article{nabi_fair_2017,
	title = {Fair Inference On Outcomes},
	url = {http://arxiv.org/abs/1705.10378},
	abstract = {In this paper, we consider the problem of fair statistical inference involving outcome variables. Examples include classification and regression problems, and estimating treatment effects in randomized trials or observational data. The issue of fairness arises in such problems where some covariates or treatments are "sensitive," in the sense of having potential of creating discrimination. In this paper, we argue that the presence of discrimination can be formalized in a sensible way as the presence of an effect of a sensitive covariate on the outcome along certain causal pathways, a view which generalizes (Pearl, 2009). A fair outcome model can then be learned by solving a constrained optimization problem. We discuss a number of complications that arise in classical statistical inference due to this view and provide workarounds based on recent work in causal and semi-parametric inference.},
	journaltitle = {{arXiv}:1705.10378 [stat]},
	author = {Nabi, Razieh and Shpitser, Ilya},
	urldate = {2018-06-08},
	date = {2017-05-29},
	eprinttype = {arxiv},
	eprint = {1705.10378},
	keywords = {Include-In-Literature-Review, Section:Causality, Technical}
}

@article{kusner_counterfactual_2017,
	title = {Counterfactual Fairness},
	url = {http://arxiv.org/abs/1703.06856},
	abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
	journaltitle = {{arXiv}:1703.06856 [cs, stat]},
	author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
	urldate = {2018-06-08},
	date = {2017-03-20},
	eprinttype = {arxiv},
	eprint = {1703.06856},
	keywords = {Include-In-Literature-Review, Section:Causality, Technical}
}

@article{kim_multiaccuracy:_2018,
	title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
	url = {http://arxiv.org/abs/1805.12317},
	shorttitle = {Multiaccuracy},
	abstract = {Machine learning predictors are successfully deployed in applications ranging from disease diagnosis, to predicting credit scores, to image recognition. Even when the overall accuracy is high, the predictions often have systematic biases that harm specific subgroups, especially for subgroups that are minorities in the training data. We develop a rigorous framework of multiaccuracy auditing and post-processing to improve predictor accuracies across identifiable subgroups. Our algorithm, {MultiaccuracyBoost}, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing. We prove guarantees on the convergence rate of the algorithm and show that it improves overall accuracy at each step. Importantly, if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We demonstrate the effectiveness of this approach on diverse applications in image classification, finance, and population health. {MultiaccuracyBoost} can improve subpopulation accuracy (e.g. for `black women') even when the sensitive features (e.g. `race', `gender') are not known to the algorithm.},
	journaltitle = {{arXiv}:1805.12317 [cs, stat]},
	author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
	urldate = {2018-06-06},
	date = {2018-05-31},
	eprinttype = {arxiv},
	eprint = {1805.12317},
	keywords = {Include-In-Literature-Review, Section:Fairness-For-Multiple-Subgroups, Technical}
}

@online{hardt_approaching_2016,
	title = {Approaching fairness in machine learning},
	url = {http://blog.mrtz.org/2016/09/06/approaching-fairness.html},
	titleaddon = {Moody Rd},
	type = {Blog},
	author = {Hardt, Moritz},
	urldate = {2018-05-30},
	date = {2016-09-06},
	keywords = {Technical}
}

@article{feldman_certifying_2014,
	title = {Certifying and removing disparate impact},
	url = {http://arxiv.org/abs/1412.3756},
	abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender, religious practice) and an explicit description of the process. When the process is implemented using computers, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the algorithm, we propose making inferences based on the data the algorithm uses. We make four contributions to this problem. First, we link the legal notion of disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on analyzing the information leakage of the protected class from the other data attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
	journaltitle = {{arXiv}:1412.3756 [cs, stat]},
	author = {Feldman, Michael and Friedler, Sorelle and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	urldate = {2018-05-29},
	date = {2014-12-11},
	eprinttype = {arxiv},
	eprint = {1412.3756},
	keywords = {Include-In-Literature-Review, Section:Classifiers, Technical}
}

@inproceedings{dwork_fairness_2012,
	location = {New York, {NY}, {USA}},
	title = {Fairness Through Awareness},
	isbn = {978-1-4503-1115-1},
	url = {http://doi.acm.org/10.1145/2090236.2090255},
	doi = {10.1145/2090236.2090255},
	series = {{ITCS} '12},
	abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
	pages = {214--226},
	booktitle = {Proceedings of the 3rd Innovations in Theoretical Computer Science Conference},
	publisher = {{ACM}},
	author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
	urldate = {2018-05-30},
	date = {2012},
	keywords = {Include-In-Literature-Review, Read-{MYang}, Section:Fairness-Definitions, Technical}
}

@article{liu_delayed_2018,
	title = {Delayed Impact of Fair Machine Learning},
	url = {http://arxiv.org/abs/1803.04383},
	abstract = {Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.},
	journaltitle = {{arXiv}:1803.04383 [cs, stat]},
	author = {Liu, Lydia T. and Dean, Sarah and Rolf, Esther and Simchowitz, Max and Hardt, Moritz},
	urldate = {2018-06-05},
	date = {2018-03-12},
	eprinttype = {arxiv},
	eprint = {1803.04383},
	keywords = {Include-In-Literature-Review, Section:Beyond-Observational-Fairness, Technical}
}

@article{kearns_preventing_2017,
	title = {Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness},
	url = {http://arxiv.org/abs/1711.05144},
	shorttitle = {Preventing Fairness Gerrymandering},
	abstract = {The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent "fairness gerrymandering", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.},
	journaltitle = {{arXiv}:1711.05144 [cs]},
	author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
	urldate = {2018-06-07},
	date = {2017-11-14},
	eprinttype = {arxiv},
	eprint = {1711.05144},
	keywords = {Include-In-Literature-Review, Read-{MYang}, Section:Fairness-For-Multiple-Subgroups, Technical}
}

@article{kallus_residual_2018,
	title = {Residual Unfairness in Fair Machine Learning from Prejudiced Data},
	url = {http://arxiv.org/abs/1806.02887},
	abstract = {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a "bias in, bias out" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk ({SQF}) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.},
	journaltitle = {{arXiv}:1806.02887 [cs, stat]},
	author = {Kallus, Nathan and Zhou, Angela},
	urldate = {2018-06-28},
	date = {2018-06-07},
	eprinttype = {arxiv},
	eprint = {1806.02887},
	keywords = {Computer Science - Machine Learning, Include-In-Literature-Review, Statistics - Machine Learning, Technical}
}

@article{hebert-johnson_calibration_2017,
	title = {Calibration for the (Computationally-Identifiable) Masses},
	url = {http://arxiv.org/abs/1711.08513},
	abstract = {As algorithms increasingly inform and influence decisions made about individuals, it becomes increasingly important to address concerns that these algorithms might be discriminatory. The output of an algorithm can be discriminatory for many reasons, most notably: (1) the data used to train the algorithm might be biased (in various ways) to favor certain populations over others; (2) the analysis of this training data might inadvertently or maliciously introduce biases that are not borne out in the data. This work focuses on the latter concern. We develop and study multicalbration -- a new measure of algorithmic fairness that aims to mitigate concerns about discrimination that is introduced in the process of learning a predictor from data. Multicalibration guarantees accurate (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. We think of the class as being quite rich; in particular, it can contain many overlapping subgroups of a protected group. We show that in many settings this strong notion of protection from discrimination is both attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present new algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and draw new connections to computational learning models such as agnostic learning.},
	journaltitle = {{arXiv}:1711.08513 [cs, stat]},
	author = {Hébert-Johnson, Úrsula and Kim, Michael P. and Reingold, Omer and Rothblum, Guy N.},
	urldate = {2018-06-19},
	date = {2017-11-22},
	eprinttype = {arxiv},
	eprint = {1711.08513},
	keywords = {Include-In-Literature-Review, Section:Fairness-For-Multiple-Subgroups, Technical}
}

@article{chouldechova_fair_2017,
	title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
	volume = {5},
	issn = {2167-6461, 2167-647X},
	url = {http://online.liebertpub.com/doi/10.1089/big.2016.0047},
	doi = {10.1089/big.2016.0047},
	shorttitle = {Fair Prediction with Disparate Impact},
	abstract = {Recidivism prediction instruments ({RPI}’s) provide decision makers with an assessment of the likelihood that a criminal defendant will reoﬀend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisﬁed when recidivism prevalence diﬀers across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance.},
	pages = {153--163},
	number = {2},
	journaltitle = {Big Data},
	author = {Chouldechova, Alexandra},
	urldate = {2018-06-01},
	date = {2017-06},
	langid = {english},
	keywords = {Include-In-Literature-Review, Section:Impossibility-Theorems, Technical}
}

@article{corbett-davies_algorithmic_2017,
	title = {Algorithmic decision making and the cost of fairness},
	url = {http://arxiv.org/abs/1701.08230},
	doi = {10.1145/3097983.309809},
	abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques recently have been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
	journaltitle = {{arXiv}:1701.08230 [cs, stat]},
	author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
	urldate = {2018-06-25},
	date = {2017-01-27},
	eprinttype = {arxiv},
	eprint = {1701.08230},
	keywords = {Include-In-Literature-Review, Read-{MYang}, Section:Problems-With-Observational-Fairness, Technical}
}
@article{hardt_equality_2016,
	title = {Equality of Opportunity in Supervised Learning},
	url = {http://arxiv.org/abs/1610.02413},
	abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of {FICO} credit scores.},
	journaltitle = {{arXiv}:1610.02413 [cs]},
	author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
	urldate = {2018-05-30},
	date = {2016-10-07},
	eprinttype = {arxiv},
	eprint = {1610.02413},
	keywords = {Include-In-Literature-Review, Read-{MYang}, Section:Fairness-Definitions, Technical}
}

@article{simonite_artificial_2017,
	title = {Artificial Intelligence Seeks An Ethical Conscience},
	issn = {1059-1028},
	url = {https://www.wired.com/story/artificial-intelligence-seeks-an-ethical-conscience/},
	abstract = {Some {AI} researchers are concerned by the field's power, and its ability to cause harm.},
	journaltitle = {Wired},
	author = {Simonite, Tom},
	urldate = {2018-09-22},
	date = {2017-12-07},
	keywords = {Artificial Intelligence, {NonTechnical}, artificial intelligence, ethics, machine learning}
}

@online{green_putting_2018,
	title = {Putting the J(ustice) in {FAT}},
	url = {https://medium.com/berkman-klein-center/putting-the-j-ustice-in-fat-28da2b8eae6d},
	abstract = {In this Medium post, {BKC} Fellow Ben Green calls for the disentanglement of the concepts of unfairness and injustice within the Fairness, Accountability, and Transparency movement.

"[H]ighlighting fairness at the expense of justice reinforces the dangerous presumption that technical systems are politically-neutral tools. While this view is appealing to computer scientists, it denies the well-established fact that algorithms exist in a social, political, and legal context where technical systems can bolster power, alter perceptions, and shroud behavior."},
	titleaddon = {Berkman Klein Center Collection},
	author = {Green, Ben},
	urldate = {2018-09-22},
	date = {2018-02-26},
	keywords = {{NonTechnical}}
}

@article{berk_fairness_2017,
	title = {Fairness in Criminal Justice Risk Assessments: The State of the Art},
	url = {http://arxiv.org/abs/1703.09207},
	shorttitle = {Fairness in Criminal Justice Risk Assessments},
	abstract = {Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy. Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments. Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy. Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.},
	journaltitle = {{arXiv}:1703.09207 [stat]},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
	urldate = {2018-09-22},
	date = {2017-03-27},
	eprinttype = {arxiv},
	eprint = {1703.09207},
	keywords = {{NonTechnical}, Statistics - Machine Learning}
}

@article{bantilan_themis-ml:_2017,
	title = {Themis-ml: A Fairness-aware Machine Learning Interface for End-to-end Discrimination Discovery and Mitigation},
	url = {http://arxiv.org/abs/1710.06921},
	shorttitle = {Themis-ml},
	abstract = {As more industries integrate machine learning into socially sensitive decision processes like hiring, loan-approval, and parole-granting, we are at risk of perpetuating historical and contemporary socioeconomic disparities. This is a critical problem because on the one hand, organizations who use but do not understand the discriminatory potential of such systems will facilitate the widening of social disparities under the assumption that algorithms are categorically objective. On the other hand, the responsible use of machine learning can help us measure, understand, and mitigate the implicit historical biases in socially sensitive data by expressing implicit decision-making mental models in terms of explicit statistical models. In this paper we specify, implement, and evaluate a "fairness-aware" machine learning interface called themis-ml, which is intended for use by individual data scientists and engineers, academic research teams, or larger product teams who use machine learning in production systems.},
	journaltitle = {{arXiv}:1710.06921 [cs]},
	author = {Bantilan, Niels},
	urldate = {2018-09-22},
	date = {2017-10-18},
	eprinttype = {arxiv},
	eprint = {1710.06921},
	keywords = {Computer Science - Computers and Society, Technical}
}

@article{barocas_big_2017,
	title = {Big Data, Data Science, and Civil Rights},
	url = {http://arxiv.org/abs/1706.03102},
	abstract = {Advances in data analytics bring with them civil rights implications. Data-driven and algorithmic decision making increasingly determine how businesses target advertisements to consumers, how police departments monitor individuals or groups, how banks decide who gets a loan and who does not, how employers hire, how colleges and universities make admissions and financial aid decisions, and much more. As data-driven decisions increasingly affect every corner of our lives, there is an urgent need to ensure they do not become instruments of discrimination, barriers to equality, threats to social justice, and sources of unfairness. In this paper, we argue for a concrete research agenda aimed at addressing these concerns, comprising five areas of emphasis: (i) Determining if models and modeling procedures exhibit objectionable bias; (ii) Building awareness of fairness into machine learning methods; (iii) Improving the transparency and control of data- and model-driven decision making; (iv) Looking beyond the algorithm(s) for sources of bias and unfairness-in the myriad human decisions made during the problem formulation and modeling process; and (v) Supporting the cross-disciplinary scholarship necessary to do all of that well.},
	journaltitle = {{arXiv}:1706.03102 [cs]},
	author = {Barocas, Solon and Bradley, Elizabeth and Honavar, Vasant and Provost, Foster},
	urldate = {2018-09-22},
	date = {2017-06-09},
	eprinttype = {arxiv},
	eprint = {1706.03102},
	keywords = {Computer Science - Computers and Society, Legal}
}

@article{menon_cost_2018,
	title = {The Cost of Fairness in Binary Classiﬁcation},
	abstract = {Binary classiﬁers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive, e.g. race. We study the inherent tradeoﬀs in learning classiﬁers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairnessaware classiﬁers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such costsensitive fairness measures, the optimal classiﬁer is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoﬀ between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.},
	pages = {12},
	author = {Menon, Aditya Krishna and Williamson, Robert C},
	date = {2018-01},
	langid = {english},
	keywords = {Technical}
}

@article{lehr_playing_2017,
	title = {Playing with the Data: What Legal Scholars Should Learn About Machine Learning},
	volume = {51},
	pages = {65},
	author = {Lehr, David and Ohm, Paul},
	date = {2017},
	langid = {english},
	keywords = {Legal}
}

@article{citron_scored_2014,
	title = {The Scored Society: Due Process for Automated Predictions},
	volume = {89},
	abstract = {Big Data is increasingly mined to rank and rate individuals. Predictive algorithms assess whether we are good credit risks, desirable employees, reliable tenants, valuable customers—or deadbeats, shirkers, menaces, and “wastes of time.” Crucial opportunities are on the line, including the ability to obtain loans, work, housing, and insurance. Though automated scoring is pervasive and consequential, it is also opaque and lacking oversight. In one area where regulation does prevail—credit—the law focuses on credit history, not the derivation of scores from data.},
	pages = {33},
	journaltitle = {Washington Law Review},
	author = {Citron, Danielle Keats and Pasquale, Frank},
	date = {2014},
	langid = {english},
	keywords = {Legal}
}

@article{citron_technological_2007,
	title = {Technological Due Process},
	volume = {85},
	abstract = {Distinct and complementary procedures for adjudication and rulemaking lie at the heart of twentieth-century administrative law. Due process requires agencies to provide individuals notice and an opportunity to be heard. Through public rulemaking, agencies can foreclose policy issues that individuals might otherwise raise in adjudication. One system allows for focused advocacy; the other features broad participation. Each procedural regime compensates for the normative limits of the other. Both depend on clear statements of reason.},
	pages = {66},
	author = {Citron, Danielle Keats},
	date = {2007},
	langid = {english},
	keywords = {Legal}
}

@article{zafar_parity_2017,
	title = {From Parity to Preference-based Notions of Fairness in Classification},
	url = {http://arxiv.org/abs/1707.00010},
	abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
	journaltitle = {{arXiv}:1707.00010 [cs, stat]},
	author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P. and Weller, Adrian},
	urldate = {2018-09-22},
	date = {2017-06-30},
	eprinttype = {arxiv},
	eprint = {1707.00010},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Technical}
}

@incollection{russell_when_2017,
	title = {When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness},
	url = {http://papers.nips.cc/paper/7220-when-worlds-collide-integrating-different-counterfactual-assumptions-in-fairness.pdf},
	shorttitle = {When Worlds Collide},
	pages = {6414--6423},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Russell, Chris and Kusner, Matt J and Loftus, Joshua and Silva, Ricardo},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	urldate = {2018-09-22},
	date = {2017},
	keywords = {Technical}
}

@article{rahwan_society---loop:_2018,
	title = {Society-in-the-Loop: Programming the Algorithmic Social Contract},
	volume = {20},
	issn = {1388-1957, 1572-8439},
	url = {http://arxiv.org/abs/1707.07232},
	doi = {10.1007/s10676-017-9430-8},
	shorttitle = {Society-in-the-Loop},
	abstract = {Recent rapid advances in Artificial Intelligence ({AI}) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. Many commentators, scholars, and policy-makers now call for ensuring that algorithms governing our lives are transparent, fair, and accountable. Here, I propose a conceptual framework for the regulation of {AI} and algorithmic systems. I argue that we need tools to program, debug and maintain an algorithmic social contract, a pact between various human stakeholders, mediated by machines. To achieve this, we can adapt the concept of human-in-the-loop ({HITL}) from the fields of modeling and simulation, and interactive machine learning. In particular, I propose an agenda I call society-in-the-loop ({SITL}), which combines the {HITL} control paradigm with mechanisms for negotiating the values of various stakeholders affected by {AI} systems, and monitoring compliance with the agreement. In short, `{SITL} = {HITL} + Social Contract.'},
	pages = {5--14},
	number = {1},
	journaltitle = {Ethics and Information Technology},
	author = {Rahwan, Iyad},
	urldate = {2018-09-22},
	date = {2018-03},
	eprinttype = {arxiv},
	eprint = {1707.07232},
	keywords = {Computer Science - Computers and Society, K.4.1, K.5.2, Technical}
}

@article{kleinberg_inherent_2016,
	title = {Inherent Trade-Offs in the Fair Determination of Risk Scores},
	url = {http://arxiv.org/abs/1609.05807},
	abstract = {Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
	journaltitle = {{arXiv}:1609.05807 [cs, stat]},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	urldate = {2018-09-22},
	date = {2016-09-19},
	eprinttype = {arxiv},
	eprint = {1609.05807},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning, Technical}
}

@article{chouldechova_fairer_2017,
	title = {Fairer and more accurate, but for whom?},
	url = {http://arxiv.org/abs/1707.00046},
	abstract = {Complex statistical machine learning models are increasingly being used or considered for use in high-stakes decision-making pipelines in domains such as financial services, health care, criminal justice and human services. These models are often investigated as possible improvements over more classical tools such as regression models or human judgement. While the modeling approach may be new, the practice of using some form of risk assessment to inform decisions is not. When determining whether a new model should be adopted, it is therefore essential to be able to compare the proposed model to the existing approach across a range of task-relevant accuracy and fairness metrics. Looking at overall performance metrics, however, may be misleading. Even when two models have comparable overall performance, they may nevertheless disagree in their classifications on a considerable fraction of cases. In this paper we introduce a model comparison framework for automatically identifying subgroups in which the differences between models are most pronounced. Our primary focus is on identifying subgroups where the models differ in terms of fairness-related quantities such as racial or gender disparities. We present experimental results from a recidivism prediction task and a hypothetical lending example.},
	journaltitle = {{arXiv}:1707.00046 [cs, stat]},
	author = {Chouldechova, Alexandra and G'Sell, Max},
	urldate = {2018-09-22},
	date = {2017-06-30},
	eprinttype = {arxiv},
	eprint = {1707.00046},
	keywords = {Computer Science - Computers and Society, Statistics - Applications, Statistics - Machine Learning, Technical}
}

@article{brantingham_does_2018,
	title = {Does Predictive Policing Lead to Biased Arrests? Results From a Randomized Controlled Trial},
	volume = {5},
	issn = {null},
	url = {https://doi.org/10.1080/2330443X.2018.1438940},
	doi = {10.1080/2330443X.2018.1438940},
	shorttitle = {Does Predictive Policing Lead to Biased Arrests?},
	abstract = {Racial bias in predictive policing algorithms has been the focus of a number of recent news articles, statements of concern by several national organizations (e.g., the {ACLU} and {NAACP}), and simulation-based research. There is reasonable concern that predictive algorithms encourage directed police patrols to target minority communities with discriminatory consequences for minority individuals. However, to date there have been no empirical studies on the bias of predictive algorithms used for police patrol. Here, we test for such biases using arrest data from the Los Angeles predictive policing experiments. We find that there were no significant differences in the proportion of arrests by racial-ethnic group between control and treatment conditions. We find that the total numbers of arrests at the division level declined or remained unchanged during predictive policing deployments. Arrests were numerically higher at the algorithmically predicted locations. When adjusted for the higher overall crime rate at algorithmically predicted locations, however, arrests were lower or unchanged.},
	pages = {1--6},
	number = {1},
	journaltitle = {Statistics and Public Policy},
	author = {Brantingham, P. Jeffrey and Valasik, Matthew and Mohler, George O.},
	urldate = {2018-09-22},
	date = {2018-01-01},
	keywords = {Civil liberties, Crime forecasting, {NonTechnical}, Police bias}
}

@article{olhede_when_2017,
	title = {When algorithms go wrong, who is liable?},
	volume = {14},
	rights = {© 2017 The Royal Statistical Society},
	issn = {1740-9713},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2017.01085.x},
	doi = {10.1111/j.1740-9713.2017.01085.x},
	abstract = {As automated decisions affect more and different areas of our lives, we are faced with ethical and legal questions that are likely to change the way we think about algorithms and the law. By Sofia Olhede and Patrick Wolfe},
	pages = {8--9},
	number = {6},
	journaltitle = {Significance},
	author = {Olhede, Sofia and Wolfe, Patrick},
	urldate = {2018-09-22},
	date = {2017-12-01},
	langid = {english},
	keywords = {Legal}
}

@article{goodman_algorithms_2018,
	title = {Algorithms and civil rights: Understanding the issues},
	url = {http://www.fedbar.org/Image-Library/Sections-and-Divisions/Civil-Rights/Civil-Rights-Winter-2018.aspx},
	pages = {3--4},
	journaltitle = {Civil Rights Insider},
	author = {Goodman, Rachel},
	urldate = {2018-09-22},
	date = {2018},
	keywords = {Legal}
}

@article{barocas_big_2016,
	title = {Big Data's Disparate Impact},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2477899},
	doi = {10.2139/ssrn.2477899},
	journaltitle = {{SSRN} Electronic Journal},
	author = {Barocas, Solon and Selbst, Andrew D.},
	urldate = {2018-09-22},
	date = {2016},
	langid = {english},
	keywords = {Legal}
}

@book{oneil_weapons_2016,
	location = {New York},
	edition = {First edition},
	title = {Weapons of math destruction: how big data increases inequality and threatens democracy},
	isbn = {978-0-553-41881-1 978-0-553-41883-5},
	shorttitle = {Weapons of math destruction},
	pagetotal = {259},
	publisher = {Crown},
	author = {O'Neil, Cathy},
	date = {2016},
	keywords = {21st century, Big data, Democracy, Mathematical models Moral and ethical aspects, {NonTechnical}, Political aspects, Social aspects, Social conditions, Social indicators, United States}
}

@article{crawford_can_2016,
	title = {Can an Algorithm be Agonistic? Ten Scenes from Life in Calculated Publics},
	volume = {41},
	issn = {0162-2439, 1552-8251},
	url = {http://journals.sagepub.com/doi/10.1177/0162243915589635},
	doi = {10.1177/0162243915589635},
	shorttitle = {Can an Algorithm be Agonistic?},
	pages = {77--92},
	number = {1},
	journaltitle = {Science, Technology, \& Human Values},
	author = {Crawford, Kate},
	urldate = {2018-09-22},
	date = {2016-01},
	langid = {english},
	keywords = {{NonTechnical}}
}

@article{campolo2017ai,
	title = {{AI} Now 2017 Report},
	journaltitle = {{AI} Now Institute at New York University},
	author = {Campolo, Alex and Sanfilippo, Madelyn and Whittaker, Meredith and Crawford, Kate},
	date = {2017},
	keywords = {{NonTechnical}}
}

@video{crawford_trouble_2017,
	location = {{NIPS} 2017},
	title = {The Trouble with Bias},
	url = {https://www.youtube.com/watch?v=fMym_BKWQzk},
	author = {Crawford, Kate},
	urldate = {2018-09-22},
	date = {2017-12-05},
	keywords = {Kate Crawford, {NIPS}2017, {NonTechnical}, ai, artificial intelligence, deep learning, deepmind, driverless cars, machine learning, robotics, robots, self-driving cars, singularity}
}

@article{ensign_decision_2018,
	title = {Decision making with limited feedback: Error bounds for predictive policing and recidivism prediction},
	volume = {83},
	abstract = {In this paper, we focus on the problems of recidivism prediction and predictive policing. We present the ﬁrst algorithms with provable regret for these problems, by showing that both problems (and others like these) can be abstracted into a general reinforcement learning framework called partial monitoring. We also discuss the policy implications of these solutions.},
	pages = {9},
	journaltitle = {Proceedings of Machine Learning Research},
	author = {Ensign, Danielle and Frielder, Sorelle A and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	date = {2018},
	langid = {english},
	keywords = {Technical}
}

@article{elzayn_fair_2018,
	title = {Fair Algorithms for Learning in Allocation Problems},
	url = {http://arxiv.org/abs/1808.10549},
	abstract = {Settings such as lending and policing can be modeled by a centralized agent allocating a resource (loans or police officers) amongst several groups, in order to maximize some objective (loans given that are repaid or criminals that are apprehended). Often in such problems fairness is also a concern. A natural notion of fairness, based on general principles of equality of opportunity, asks that conditional on an individual being a candidate for the resource, the probability of actually receiving it is approximately independent of the individual's group. In lending this means that equally creditworthy individuals in different racial groups have roughly equal chances of receiving a loan. In policing it means that two individuals committing the same crime in different districts would have roughly equal chances of being arrested. We formalize this fairness notion for allocation problems and investigate its algorithmic consequences. Our main technical results include an efficient learning algorithm that converges to an optimal fair allocation even when the frequency of candidates (creditworthy individuals or criminals) in each group is unknown. The algorithm operates in a censored feedback model in which only the number of candidates who received the resource in a given allocation can be observed, rather than the true number of candidates. This models the fact that we do not learn the creditworthiness of individuals we do not give loans to nor learn about crimes committed if the police presence in a district is low. As an application of our framework, we consider the predictive policing problem. The learning algorithm is trained on arrest data gathered from its own deployments on previous days, resulting in a potential feedback loop that our algorithm provably overcomes. We empirically investigate the performance of our algorithm on the Philadelphia Crime Incidents dataset.},
	journaltitle = {{arXiv}:1808.10549 [cs, stat]},
	author = {Elzayn, Hadi and Jabbari, Shahin and Jung, Christopher and Kearns, Michael and Neel, Seth and Roth, Aaron and Schutzman, Zachary},
	urldate = {2018-09-18},
	date = {2018-08-30},
	eprinttype = {arxiv},
	eprint = {1808.10549},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Technical}
}

@article{ensign_runaway_2017,
	title = {Runaway Feedback Loops in Predictive Policing},
	url = {http://arxiv.org/abs/1706.09847},
	abstract = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which {\textbackslash}emph\{reported\} incidents of crime (those reported by residents) and {\textbackslash}emph\{discovered\} incidents of crime (i.e. those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.},
	journaltitle = {{arXiv}:1706.09847 [cs, stat]},
	author = {Ensign, Danielle and Friedler, Sorelle A. and Neville, Scott and Scheidegger, Carlos and Venkatasubramanian, Suresh},
	urldate = {2018-09-18},
	date = {2017-06-29},
	eprinttype = {arxiv},
	eprint = {1706.09847},
	keywords = {Computer Science - Computers and Society, Statistics - Machine Learning, Technical}
}

@article{mohler_self-exciting_2011,
	title = {Self-Exciting Point Process Modeling of Crime},
	volume = {106},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.ap09546},
	doi = {10.1198/jasa.2011.ap09546},
	pages = {100--108},
	number = {493},
	journaltitle = {Journal of the American Statistical Association},
	author = {Mohler, G. O. and Short, M. B. and Brantingham, P. J. and Schoenberg, F. P. and Tita, G. E.},
	urldate = {2018-09-18},
	date = {2011-03},
	langid = {english},
	keywords = {Technical}
}

@article{lum_predict_2016,
	title = {To predict and serve?},
	volume = {13},
	rights = {© 2016 The Royal Statistical Society},
	issn = {1740-9713},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2016.00960.x},
	doi = {10.1111/j.1740-9713.2016.00960.x},
	abstract = {Predictive policing systems are used increasingly by law enforcement to try to prevent crime before it occurs. But what happens when these systems are trained using biased data? Kristian Lum and William Isaac consider the evidence – and the social consequences},
	pages = {14--19},
	number = {5},
	journaltitle = {Significance},
	author = {Lum, Kristian and Isaac, William},
	urldate = {2018-09-18},
	date = {2016-10-01},
	keywords = {Technical}
}

@article{canonne_testing_2017,
	title = {Testing Conditional Independence of Discrete Distributions},
	url = {http://arxiv.org/abs/1711.11560},
	abstract = {We study the problem of testing {\textbackslash}emph\{conditional independence\} for discrete distributions. Specifically, given samples from a discrete random variable \$(X, Y, Z)\$ on domain \$[{\textbackslash}ell\_1]{\textbackslash}times[{\textbackslash}ell\_2] {\textbackslash}times [n]\$, we want to distinguish, with probability at least \$2/3\$, between the case that \$X\$ and \$Y\$ are conditionally independent given \$Z\$ from the case that \$(X, Y, Z)\$ is \${\textbackslash}epsilon\$-far, in \${\textbackslash}ell\_1\$-distance, from every distribution that has this property. Conditional independence is a concept of central importance in probability and statistics with a range of applications in various scientific domains. As such, the statistical task of testing conditional independence has been extensively studied in various forms within the statistics and econometrics communities for nearly a century. Perhaps surprisingly, this problem has not been previously considered in the framework of distribution property testing and in particular no tester with sublinear sample complexity is known, even for the important special case that the domains of \$X\$ and \$Y\$ are binary. The main algorithmic result of this work is the first conditional independence tester with \{{\textbackslash}em sublinear\} sample complexity for discrete distributions over \$[{\textbackslash}ell\_1]{\textbackslash}times[{\textbackslash}ell\_2] {\textbackslash}times [n]\$. To complement our upper bounds, we prove information-theoretic lower bounds establishing that the sample complexity of our algorithm is optimal, up to constant factors, for a number of settings. Specifically, for the prototypical setting when \${\textbackslash}ell\_1, {\textbackslash}ell\_2 = O(1)\$, we show that the sample complexity of testing conditional independence (upper bound and matching lower bound) is {\textbackslash}[ {\textbackslash}Theta{\textbackslash}left(\{{\textbackslash}max{\textbackslash}left(n{\textasciicircum}\{1/2\}/{\textbackslash}epsilon{\textasciicircum}2,{\textbackslash}min{\textbackslash}left(n{\textasciicircum}\{7/8\}/{\textbackslash}epsilon,n{\textasciicircum}\{6/7\}/{\textbackslash}epsilon{\textasciicircum}\{8/7\}{\textbackslash}right){\textbackslash}right)\}{\textbackslash}right){\textbackslash},. {\textbackslash}]},
	journaltitle = {{arXiv}:1711.11560 [cs, math, stat]},
	author = {Canonne, Clément L. and Diakonikolas, Ilias and Kane, Daniel M. and Stewart, Alistair},
	urldate = {2018-06-18},
	date = {2017-11-30},
	eprinttype = {arxiv},
	eprint = {1711.11560},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Conditional Independence, Mathematics - Probability, Mathematics - Statistics Theory, Statistical Tests, Statistics}
}

@book{hughes_what_2015,
	title = {What Is Environmental History?},
	isbn = {978-0-7456-8846-6},
	url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=4182797},
	pagetotal = {187},
	publisher = {Wiley},
	author = {Hughes, J. Donald},
	urldate = {2017-02-19},
	date = {2015},
	note = {{OCLC}: 931806027}
}

@book{guldi_history_2014,
	location = {Cambridge},
	title = {The History Manifesto},
	isbn = {978-1-107-07634-1},
	url = {http://historymanifesto.cambridge.org/},
	publisher = {Cambridge University Press},
	author = {Guldi, Jo and Armitage, David},
	urldate = {2017-02-19},
	date = {2014},
	doi = {10.1017/9781139923880}
}

@online{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero Quick Start Guide},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}