%************************************************
\chapter{Conclusion}\label{ch:conclusion}
%************************************************

% Reframe the work of this paper in terms of intrinsic accuracy and fairness trade-offs (FairPol doesn't work as well as intended)

To summarize, we implemented an existing predictive policing model and assessed it for accuracy and fairness relative to a baseline measure. We found that the method was not more accurate than the baseline method nor was it fairer. We proposed a post-processing fairness task that seems to improve the fairness of results while maintaining a good amount of accuracy. We also discussed some of the normative implications of this work on the use of predictive policing.

\section{Further Work}
% \begin{itemize}
% \item Extend to multiple races
% \item Replicate tests on other datasets
% \item Replicating FairPol on a dataset that contains race data for individual
% crimes
% \item Moving beyond the "observational" notions of fairness surveyed in this
% paper
% \item Proving the NP-completeness of the fair-ranking problem
% \item Create approximation algorithms for the fair-ranking problem
% \end{itemize}
The knapsack problem and many of its variants, including the multi-dimensional knapsack problem, are known to be NP-hard problems. It might be interesting to prove that the knapsack reduction discussed in \autoref{ch:fairpol} is also NP-hard, despite the fact that there is likely to be correlation between the value of each item and the costs associated with each item.

However, we would also like to suggest future lines of computer science research that more directly promote justice and equity. While there has been a heightened interest in assessing the social impacts of various computational techniques in society, much of the scholarly work from computer scientists has been, unsurprisingly, abstract and reductive. This work has been abstract in the sense that they detach themselves from real-world usage and the complexity of algorithmic applications and reductive in the sense that they boil down complex and ambiguous concepts such as "fairness" and "equity" into single and ultimately limited equations. Our hope is that this thesis shows that not all computer scientists have to be bound by the preferences of their field.

For instance, computer scientists can begin to model how communities respond to over-policing and how that might change the calculus of a police department (this work would be similar in nature to \citet{liu_delayed_2018}). This work would interrogate whether some policing strategies are more strategy-proof than others.

Computer scientists could also consider different objective functions other than accuracy for predictive policing algorithm, such as the impact of policing on police-civilian relationships or sentiments of community safety.

Recent research on "dirty data" in predictive policing systems also complicates the results of this paper \citep{richardson_dirty_2019}. The data in criminal justice systems reflect not only incidences of crime, but also policing policy---and bias. Many crimes go unreported or unrecorded in the criminal justice system for a variety of reasons. At other times, false crimes are recorded or the number of crimes are over-reported. It may be worthwhile to study how different kinds of hypothetical systematic bias in data collection change the results of this paper. Essentially, one could conduct a robustness test by assuming that police data tend to systematically over-sample some regions and under-sample others based on demographics (as was the central thesis in \citet{lum_predict_2016}).

Finally, it would be interesting to deploy causal notions of fairness (as opposed to the observational notions discussed in \autoref{ch:fairness_primer}) in the context of predictive policing.