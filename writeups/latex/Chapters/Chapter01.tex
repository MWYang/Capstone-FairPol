%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

Algorithms play an increasingly large role in many applications, including policing and criminal justice. Consequently, it is important to scrutinize both how effectively these algorithms meet their stated goals and whether such algorithms produce unintended side effects. In this thesis, we make two contributions to the assessment of algorithms with societal implications: first, we show empirically that \pp, a well-known predictive policing algorithm, does not significantly outperform a simpler, baseline method. Briefly, \pp uses kernel density estimation to approximate a statistical model of crime that accounts for, borrowing from seismology, "aftershocks" in crime. On the other hand, the baseline method (simply counting which locations have had the largest number of crimes) is both intuitive and easily explainable to the layperson. Second, we contribute a novel post-processing modification to make predictive policing fairer (according to the definition that we will supply). This method exploits the fact that while individual grid cells may be unfair, combinations of grid cells may in the aggregate be fair.

We also discuss the normative implications of our computational results. If the purported benefits of predictive policing algorithms are overstated while simpler alternatives are available, perhaps police departments ought to use the simple alternatives instead. Algorithms are essentially policy decisions that affect the wider community, and as such, these policy decisions ought to be relatively transparent and free from negative side effects.

The remainder of this thesis will proceed as follows: first, in \autoref{ch:predpol_primer} and \autoref{ch:fairness_primer}, we introduce fundamental concepts: the \pp model of crime and different technical notions of fairness. In the latter, we also weigh the varying normative concerns that each fairness definition addresses and discuss their implications in the context of predictive policing. The novel contributions then follow. In \autoref{ch:fairpol}, we describe a computational task for making the outputs of a predictive policing algorithm fairer. Then, in \autoref{ch:results}, we compare the accuracy and fairness of \pp, the aforementioned baseline technique, and the fairness modification. Finally, we conclude with the implications of the present research and future research directions in \autoref{ch:conclusion}.

\section{An Introduction to Predictive Policing}

Predictive policing systems are intuitively attractive; instead of reacting to crime, police departments can proactively patrol regions that are likely to see criminal activity \citep{perry_predictive_2013}.\footnote{Apart from the location-based systems discussed in this report, a newer system of person-based systems is also emerging. Such systems attempt to predict specific individuals who are likely to commit a crime. The ethical implications of such systems are even more concerning. For more, see \citet{robinson_stuck_2016}.} Compared to human expert-based methods, automated systems are less costly, more efficient in processing large amounts of data, and potentially less biased. These advantages have driven an uptick in the number of predictive systems used in policing departments across the country.

Predictive policing, even when deployed by actors with good intentions, may create new issues instead of resolving them. There are several common myths and pitfalls associated with predictive policing, as described by \citet{perry_predictive_2013}. First, predictive policing is generally over-hyped relative to the actual performance of these models. Commonly, the media cites the film \emph{Minority Report} to introduce an audience to the technology of predictive policing, but such an introduction is highly misleading---algorithms are not omniscient. Rather, predictive policing algorithms are like other machine learning techniques that extrapolate from historical data to make guesses about the future. The undue optimism of predictive policing is embedded within the very name itself. As a matter of practice, it may be more accurate to describe such algorithms as "forecasting" or "data mining."

Closely related to this first myth is the notion that only a sophisticated computer model can allow policing departments to become more effective. In reality, simple heuristics may often perform quite well on the same predictive policing tasks, as this thesis will endeavor to show. Police departments with smaller budgets may not be significantly disadvantaged by the lack of access to sophisticated models.

Moreover, even a highly accurate forecast is not a panacea for crime, a complex sociological phenomenon. Accurate predictions do not automatically provide tactical utility. Just knowing where crime might occur does not suggest the best strategy for preventing or reducing crime in that area. Thus, a prediction system must be combined effectively with the other elements of a police department, including data collection, interventions, and policy.

In addition to the issues of effectiveness discussed so far, predictive policing may carry significant ethical risks. First, when algorithms are deployed in a societal context, they may possibly retrench and reinforce existing biases in the data. For example, a predictive policing algorithm could create a policing feedback loop in which the patrols visit the same areas repeatedly. The computer models then update solely on the basis of data collected by said patrols (the next section, \ref{sec:related_work}, will highlight some of the relevant computer science literature on this topic). Of more relevance to the present thesis is the question of transparency: an algorithmic decision-maker in the context of something as sensitive as policing is effectively a decision-maker. Unlike other policy-makers though, an algorithm cannot clearly articulate its policy decisions. Moreover, many of the affected members of the public may be unable to participate effectively in the discussion around an algorithm because the details of the algorithm, even if they are nominally available for inspection, are hidden behind mathematics and jargon. Critics question whether the benefits of predictive policing are worth these ethical costs.

\section{Related Work} \label{sec:related_work}
Other researchers have investigated the policing feedback loops that \pp and other predictive policing algorithms are liable to create \citep{lum_predict_2016,ensign_runaway_2017,ensign_decision_2018}. The latter two papers characterize theoretically the possibility of feedback loops and offer technical remedies. The risk for feedback loops also highlights the importance of effective and reasonable "end-to-end" policing practices, since the data collection process and not just the accuracy of predictions affects the end result. This thesis does not directly explore the possibility for feedback loops.

Reports on the effectiveness of \pp are mixed \citep{robinson_stuck_2016}. An earlier investigation by a Swiss researcher comes to similar conclusions as this report on the effectiveness of \pp relative to simple baseline techniques \citep{benslimane_etude_2014} (French language only). Moreover, real-world evidence about potential reductions in crime from \pp are not conclusive. The makers of \pp have been laudably transparent in publishing their methods and results in established scientific journals, including results from randomized-controlled trials \citep{mohler_self-exciting_2011,mohler_marked_2014,mohler_randomized_2015}. However, said work has been criticized by other researchers who are dubious that the observed improvements in \pp can be attributed to \pp itself rather than natural variations \citep{saunders_predictions_2016}. To date, the author of this report is not aware of any third-party controlled studies of the effectiveness of \pp.

Some initial work, both theoretical and empirical, has been done to try and make \pp fairer. \citet{mohler_penalized_2018} use a penalized likelihood function to learn a fair version of \pp. That work differs from the present both in the method of accomplishing fairness and in the fairness criterion considered. The fairness definition considered there, demographic parity, is deficient for an important reason that we will discuss in \autoref{ch:fairness_primer}. Moreover, because of the fairness definition chosen, there is a strong trade-off between accuracy and fairness, while the post-processing task that we suggest does not perform as poorly. Empirically, \citet{brantingham_does_2018} perform a randomized-control trial that shows no significant increase in minority arrests from usage of \pp. To the extent that there are doubts about other randomized control trials of \pp, there ought to be similar concerns about this study as well.

Finally, computer scientists have built up a rich literature for assessing and maintaining fairness in machine learning algorithms. While these solutions, like \pp, are not panaceas, they are interesting computational problems in their own right and may be useful in certain real-world contexts. The research on \emph{fair ranking} tasks is most similar to the post-processing modification introduced in this paper \citep{celis_ranking_2017,zehlike_fa*ir:_2017,singh_fairness_2018}.
% The problem scenario and fairness criterion chosen for these papers is not identical to the setup considered here however.

%% Add section for results from FAT* 2019